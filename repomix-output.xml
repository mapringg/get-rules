This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
.cursor/
  rules/
    _global.mdc
    cli-github-search.mdc
    cli-pack.mdc
    cli-worktree.mdc
    cli-wrangler.mdc
    docs-diagram.mdc
    docs-openapi-spec.mdc
    docs-prd.mdc
    docs-structure.mdc
    docs-sync.mdc
    docs-tech-stack.mdc
    pnpm-fixes.mdc
    project-todos-next.mdc
    project-update-rules.mdc
    prompt-improve.mdc
    pull-request-create.mdc
    react-rules.mdc
    scripts-create.mdc
    task-execute.mdc
    task-next.mdc
    task-plan.mdc
    zen-coding.mdc
.github/
  workflows/
    release.yml
.cursorindexingignore
.gitignore
.npmignore
.releaserc.json
cli.js
package.json
README.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".cursor/rules/prompt-improve.mdc">
---
description:
globs:
alwaysApply: false
---
# Improve the User's Prompt Following the Patterns Below

> Practical prompt patterns to help anyone get clearer, more reliable answers from an AI agent.

| # | Pattern                       | Why It Matters                                                          | Template                                                                 | Example Prompt                                                                                             |
|---|-------------------------------|-------------------------------------------------------------------------|--------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------|
| 1 | Lead with the ask             | The model reads top-down; putting the goal first stops it wandering.    | `Do X. [context]`                                                        | "Summarize this PDF in 5 bullet points. The text is below: ..."                                            |
| 2 | Repeat the key ask at the end | Long contexts sometimes truncate; an end-cap protects you.              | `...[detail]... REMEMBER: Do X.`                                         | "List pros & cons, keep it balanced. REMEMBER: 5 pros, 5 cons."                                             |
| 3 | Specify output shape          | Dictating format cuts revision loops.                                   | `Return as: 1) short title, 2) table (CSV).`                             | "Give 3 holiday ideas. Format: destination, flight-time-hrs, avg cost."                                    |
| 4 | Use clear delimiters          | Backticks/headings/XML keep sections from blending.                     | `TEXT TO ANALYSE`                                                        | "Rate the style of the text between the fences."                                                            |
| 5 | Induce step-by-step thinking  | Planning first boosts multi-step accuracy.                              | `Think step-by-step then answer.`                                        | "Solve this puzzle. Think step-by-step before giving the final move."                                      |
| 6 | Ask it to plan its workflow   | For big jobs, AI outlines tasks before doing them.                      | `First draft a plan, wait, then execute.`                                | "We're writing an e-book. ‚ù∂ Outline chapters. ‚ù∑ Wait. ‚ù∏ When I say 'go', draft chapter 1."              |
| 7 | Limit or widen knowledge sources | Controls hallucination.                                               | `Use only the info below. / Combine basic knowledge + this context.`     | "Using only the product sheet below, write FAQs."                                                          |
| 8 | Guide information retrieval   | Helps AI pick the right docs before answering.                          | `List which docs look relevant, then answer.`                            | "30 sales memos attached. 1) Name the 5 most relevant. 2) Summarise their common points."                 |
| 9 | Show a style/example          | Anchors tone, length, vocabulary.                                       | `Match the style of: <example>`                                          | "Review this gadget in the style of the sample below: 'Short, witty, 3 key facts...'"                      |
| 10| Set correction handles        | One-line fixes let you steer quickly.                                   | `If length > 150 words, shorten.`                                        | "Describe blockchain to a child. If your answer is >150 words, cut it in half."                            |
| 11| Tell it when to stop or loop  | Prevents half-finished lists or runaway essays.                         | `Keep going until you list 20 ideas, then stop.`                         | "Brainstorm webinar titles. Give exactly 12, then finish."                                                  |
| 12| Request the hidden reasoning  | Good for audits; otherwise omit to keep it short.                       | `After the answer, include a brief reasoning section.`                   | "Which of these stocks looks over-valued? Answer first; add a 2-sentence rationale below a divider."        |
</file>

<file path=".cursorindexingignore">
# Don't index SpecStory auto-save files, but allow explicit context inclusion via @ references
.specstory/**
</file>

<file path=".npmignore">
.*
!.gitignore
!.npmignore
</file>

<file path=".cursor/rules/docs-openapi-spec.mdc">
---
description:
globs:
alwaysApply: false
---
## OpenAPI Specification Management Rule

*This rule guides the creation and maintenance of the `docs/openapi.yaml` file.*

### Core Directive
Create and maintain an `docs/openapi.yaml` file, defining an OpenAPI specification (v3.0.x or 3.1.x). Operate based on information from `NOTES.md`, `docs/PRD.md`, `docs/TECH_STACK.md`, and the existing `docs/openapi.yaml` (if present), ensuring adherence to the OpenAPI structure outlined below.

### Strict Limitations
- **File Operations:** Only read `NOTES.md`, `docs/PRD.md`, `docs/TECH_STACK.md`, `docs/openapi.yaml` (or equivalents), and read/write to `docs/openapi.yaml`. Do not interact with other files.
- **Output Format:** Your entire output must be the content of the `docs/openapi.yaml` file itself.
- **Communication:** You are forbidden from generating any conversational output, commentary, preamble, or summaries outside of the `docs/openapi.yaml` file.
- **User Interaction (within `docs/openapi.yaml`):** You do not directly converse with the user. If sections of the `docs/openapi.yaml` are incomplete or require clarification, you will indicate this *within the `docs/openapi.yaml` file* using a specific YAML comment format.

### OpenAPI Specification Structure and Content Source
The `docs/openapi.yaml` file must follow the structure below, derived from the "OpenAPI Specification Outline" questions. Aim for OpenAPI version 3.0.x or 3.1.x.

#### OpenAPI Specification Outline (Source for `docs/openapi.yaml` Structure and TODOs)

1.  **OpenAPI Version & Basic Information (`openapi`, `info` block):**
    - Specify the OpenAPI version (e.g., `openapi: 3.0.3`).
    - **Title (`info.title`):** What is the official title for this API? (Source: `docs/PRD.md` - Project Name/Title. Example: "Customer Data API").
    - **Version (`info.version`):** What is the current version of this API specification (e.g., "1.0.0", "v2.1-beta")?
    - **Description (`info.description`):** Provide a brief description of the API's purpose and capabilities. (Source: `docs/PRD.md` - Project Purpose/Core Functionality. Example: "API for managing customer profiles and order history.").
    - **(Optional) Contact (`info.contact`):**
        - `name`: Contact person/team name (e.g., "API Support Team").
        - `url`: URL to a support page or contact form.
        - `email`: Contact email address (e.g., "apisupport@example.com").
    - **(Optional) License (`info.license`):**
        - `name`: SPDX license identifier (e.g., "Apache 2.0", "MIT").
        - `url`: Link to the full license text.

2.  **Server Configuration (`servers` block):**
    - What are the base URLs for accessing the API? (Source: `docs/TECH_STACK.md` - Infrastructure & Deployment. Examples: `https://api.example.com/v1`, `https://sandbox.api.example.com/v1`).
    - For each server, provide:
        - `url`: The server URL.
        - `description`: A human-readable description (e.g., "Production Server", "Sandbox Environment").

3.  **Global Security Definitions (`components.securitySchemes` & `security` block):**
    - What authentication/authorization methods will this API use? (Source: `docs/TECH_STACK.md` - Security Considerations. Examples: API Key, OAuth2, JWT Bearer Token).
    - Define each scheme under `components.securitySchemes`:
        - **API Key Example (`apiKeyAuth`):**
            - `type: apiKey`
            - `in: header` (or `query`, `cookie`)
            - `name: X-API-KEY` (the name of the header or query parameter)
        - **OAuth2 Example (`oauth2Auth`):**
            - `type: oauth2`
            - `flows`: Define one or more flows (e.g., `authorizationCode`, `clientCredentials`).
                - `authorizationCode`:
                    - `authorizationUrl: https://auth.example.com/oauth/authorize`
                    - `tokenUrl: https://auth.example.com/oauth/token`
                    - `scopes`: Define available scopes (e.g., `read:profile`, `write:orders`).
    - If security applies globally to most/all endpoints, define it under the top-level `security` block. (Example: `security: - apiKeyAuth: []`).

4.  **Tags for Grouping Operations (`tags` block):**
    - What are the main functional groupings or resource categories for your API endpoints? (Source: `docs/PRD.md` - Key Features/Modules. Examples: "Users", "Products", "Orders").
    - For each tag, provide:
        - `name`: The tag name (e.g., "UserManagement").
        - `description`: A brief explanation of the tag (e.g., "Operations related to user accounts and profiles").

5.  **Reusable Schemas (`components.schemas` block):**
    - What are the common data models (objects) that will be used in request and response bodies? (Source: `docs/PRD.md` - Key Features, Data Models; `docs/TECH_STACK.md` - Database for entities. Examples: `User`, `Product`, `Order`, `ErrorResponse`).
    - For each schema (e.g., `User`):
        - `type: object`
        - `properties`: Define its fields.
            - For each property (e.g., `id`, `username`, `email`, `status`):
                - `type`: (e.g., string, integer, boolean)
                - `format`: (e.g., uuid, email, int64) (Optional)
                - `description`: (A brief explanation of the property) (Optional)
                - `example`: (A sample value) (Optional)
</file>

<file path=".releaserc.json">
{
  "branches": ["main"],
  "plugins": [
    "@semantic-release/commit-analyzer",
    "@semantic-release/release-notes-generator",
    [
      "@semantic-release/changelog",
      {
        "changelogFile": "CHANGELOG.md"
      }
    ],
    [
      "@semantic-release/npm",
      {
        "npmPublish": true
      }
    ],
    [
      "@semantic-release/github",
      {
        "assets": [],
        "failComment": false,
        "addReleases": "bottom",
        "failTitle": "The automated release is failing üö®",
        "labels": false,
        "assignees": []
      }
    ],
    [
      "@semantic-release/git",
      {
        "assets": ["package.json", "CHANGELOG.md"],
        "message": "chore(release): ${nextRelease.version} [skip ci]\n\n${nextRelease.notes}"
      }
    ]
  ]
}
</file>

<file path="README.md">
# get-rules

An npm utility to quickly download and install the latest `.mdc` rule files for AI-assisted coding tools (like Cursor) from John Lindquist's [get-rules](https://github.com/johnlindquist/get-rules) repository.

## Installation

Install the tool globally using npm:

```bash
npm install -g get-rules
```
Or, if you prefer to use it on a per-project basis without global installation, you can use `npx`:
```bash
npx get-rules
```

## Usage

Navigate to the root directory of your project where you want to install/update the rules, and then run:

```bash
get-rules
```

This will:
1. Connect to the GitHub API to fetch the list of files in the [johnlindquist/get-rules](https://github.com/johnlindquist/get-rules) repository.
2. Identify all `.mdc` (Markdown Custom) rule files in the root of that repository.
3. Create a `.cursor/rules` directory in your current working directory (if it doesn't exist).
4. Download each `.mdc` file into `.cursor/rules`, skipping any that already exist locally. This ensures you always get the latest versions of new rules without overwriting local modifications to existing ones unless you manually delete them first.

## How it Works

This tool is a self-contained Node.js script. It directly interacts with the GitHub API to list repository contents and then downloads the necessary `.mdc` files. It does not rely on external shell scripts like `curl`, `bash`, or `powershell`.

## Contributing

Issues and pull requests are welcome! Please feel free to contribute to the [get-rules repository](https://github.com/johnlindquist/get-rules).

For the rules themselves, please contribute to the [get-rules repository](https://github.com/johnlindquist/get-rules).

## License

MIT License - Copyright (c) John Lindquist
</file>

<file path=".cursor/rules/cli-wrangler.mdc">
---
description:
globs:
alwaysApply: false
---
# Cloudflare Wrangler CLI Reference Guide

**Your Task: When interacting with Cloudflare Workers, Pages, or other Cloudflare developer platform resources via the command line, consult this reference guide for the `wrangler` CLI. Use this information to construct appropriate `wrangler` commands based on the user's objectives.**

*This document is based on `wrangler` CLI help output and provides a comprehensive overview of its commands and structure. For the most specific details on any subcommand or its options, you can also execute `wrangler <command> [subcommand] --help` in a terminal environment if available.* 

## I. Top-Level Commands

- `wrangler docs [search...]` ‚Äî Open Wrangler's command documentation in your browser.
- `wrangler init [name]` ‚Äî Initialize a basic Worker project.
- `wrangler dev [script]` ‚Äî Start a local development server for your Worker.
- `wrangler deploy [script]` ‚Äî Deploy your Worker to Cloudflare.
- `wrangler deployments` ‚Äî List and view current and past deployments.
- `wrangler rollback [version-id]` ‚Äî Rollback a Worker to a previous deployment version.
- `wrangler versions` ‚Äî List, view, upload, and deploy specific Worker versions.
- `wrangler triggers` ‚Äî Update deployment triggers [experimental].
- `wrangler delete [script]` ‚Äî Delete a Worker from Cloudflare.
- `wrangler tail [worker]` ‚Äî Start a real-time log tailing session for a Worker.
- `wrangler secret` ‚Äî Manage secrets for a Worker (deprecated for new projects, prefer `.dev.vars` or Secrets Store).
- `wrangler types [path]` ‚Äî Generate TypeScript type definitions from your Worker's configuration (`wrangler.toml`).

## II. Resource Management Commands

- `wrangler kv` ‚Äî Manage Workers KV Namespaces (key-value store).
- `wrangler queues` ‚Äî Manage Workers Queues (message queues).
- `wrangler r2` ‚Äî Manage R2 buckets and objects (object storage).
- `wrangler d1` ‚Äî Manage Workers D1 databases (serverless SQL).
- `wrangler vectorize` ‚Äî Manage Vectorize indexes (vector embeddings database).
- `wrangler hyperdrive` ‚Äî Manage Hyperdrive database accelerators.
- `wrangler cert` ‚Äî Manage client mTLS certificates and CA certificate chains [open-beta].
- `wrangler pages` ‚Äî Configure and manage Cloudflare Pages projects.
- `wrangler mtls-certificate` ‚Äî Manage certificates for mTLS connections between services.
- `wrangler pubsub` ‚Äî Manage Pub/Sub brokers for real-time messaging [private beta].
- `wrangler dispatch-namespace` ‚Äî Manage dispatch namespaces for Worker-to-Worker communication.
- `wrangler ai` ‚Äî Manage AI models and deployments on Workers AI.
- `wrangler workflows` ‚Äî Manage Cloudflare Workflows [generally available].
- `wrangler pipelines` ‚Äî Manage Cloudflare Pipelines [open-beta].
- `wrangler login` ‚Äî Authenticate `wrangler` with your Cloudflare account.
- `wrangler logout` ‚Äî Log out from Cloudflare.
- `wrangler whoami` ‚Äî Retrieve information about the currently authenticated Cloudflare user.
- `wrangler secrets-store` ‚Äî Manage secrets using the centralized Secrets Store [alpha].

---

## III. Global Flags (Applicable to most commands)

- `-c, --config <file>` ‚Äî Path to a custom `wrangler.toml` configuration file.
- `--cwd <dir>` ‚Äî Run `wrangler` as if it were started in the specified directory.
- `-e, --env <env>` ‚Äî Specify an environment to use (defined in `wrangler.toml`). Affects operations and selection of `.env`/`.dev.vars` files.
- `-h, --help` ‚Äî Show help information for the command or subcommand.
- `-v, --version` ‚Äî Show `wrangler` CLI version number.

---

## IV. Example Subcommand Structures (Illustrative)

This section shows the hierarchical structure of some common resource management commands. Use `wrangler <command> <subcommand> --help` for full options at each level.

### `wrangler kv` (Workers KV)
- `wrangler kv:namespace create <NAMESPACE_NAME>`
- `wrangler kv:namespace list`
- `wrangler kv:namespace delete --namespace-id <ID>`
- `wrangler kv:key put <KEY> [VALUE] --namespace-id <ID> [--path --preview]`
- `wrangler kv:key list --namespace-id <ID>`
- `wrangler kv:key get <KEY> --namespace-id <ID>`
- `wrangler kv:key delete <KEY> --namespace-id <ID>`
- `wrangler kv:bulk put <FILENAME.JSON> --namespace-id <ID>` (File format: `[{"key":"foo", "value":"bar"},...]`)
- `wrangler kv:bulk delete <FILENAME.JSON> --namespace-id <ID>` (File format: `["foo", "bar", ...]`)

### `wrangler queues` (Workers Queues)
- `wrangler queues list`
- `wrangler queues create <QUEUE_NAME>`
- `wrangler queues update <QUEUE_NAME> [--consumer <WORKER_NAME> --consumer-batch-size <SIZE> ...]`
- `wrangler queues delete <QUEUE_NAME>`
- `wrangler queues info <QUEUE_NAME>`
- `wrangler queues consumer add <QUEUE_NAME> <WORKER_NAME> [--batch-size <SIZE> ...]`
- `wrangler queues consumer remove <QUEUE_NAME> <WORKER_NAME>`
- `wrangler queues purge <QUEUE_NAME>`

### `wrangler r2` (R2 Object Storage)
- `wrangler r2 object get <BUCKET_NAME>/<OBJECT_KEY> [--file <OUTPUT_FILE>]`
- `wrangler r2 object put <BUCKET_NAME>/<OBJECT_KEY> --file <SOURCE_FILE> [--content-type <MIME>]`
- `wrangler r2 object delete <BUCKET_NAME>/<OBJECT_KEY>`
- `wrangler r2 bucket create <BUCKET_NAME>`
- `wrangler r2 bucket list`
- `wrangler r2 bucket info <BUCKET_NAME>`
- `wrangler r2 bucket delete <BUCKET_NAME>`

### `wrangler d1` (D1 Serverless SQL)
- `wrangler d1 list`
- `wr wrangler d1 info <DATABASE_NAME>`
- `wrangler d1 create <DATABASE_NAME>`
- `wrangler d1 delete <DATABASE_NAME>`
- `wrangler d1 execute <DATABASE_NAME> --command "SELECT * FROM users;"`
- `wrangler d1 execute <DATABASE_NAME> --file ./migrations/0001_init.sql`
- `wrangler d1 backup list <DATABASE_NAME>`
- `wrangler d1 backup create <DATABASE_NAME>`
- `wrangler d1 backup restore <DATABASE_NAME> <BACKUP_ID>`
- `wrangler d1 migrations list <DATABASE_NAME>`
- `wrangler d1 migrations apply <DATABASE_NAME>`

### `wrangler vectorize` (Vectorize Database)
- `wrangler vectorize create <INDEX_NAME> --dimensions <NUMBER> --metric <METRIC_TYPE>`
- `wrangler vectorize delete <INDEX_NAME>`
- `wrangler vectorize get <INDEX_NAME>`
- `wrangler vectorize list`
- `wrangler vectorize query <INDEX_NAME> --vector "[0.1, 0.2, ...]" [--top-k <NUMBER>]`
- `wrangler vectorize insert <INDEX_NAME> --file <VECTORS_FILE.JSON>` (File: `[{"id":"vec1", "values":[...], "metadata":{...}}, ...]`) 

*(Other subcommand trees like `hyperdrive`, `pages`, `ai`, etc., follow similar patterns. Consult `wrangler <command> --help` for specifics.)*

---

**REMEMBER: This guide is your primary reference for `wrangler` CLI commands, their structure, and common resource management tasks. Use it to accurately construct commands to interact with the Cloudflare developer platform. For detailed options on any specific command, the `--help` flag is your best friend (e.g., `wrangler r2 bucket create --help`).**
</file>

<file path=".cursor/rules/docs-diagram.mdc">
---
description:
globs:
alwaysApply: false
---
# Create GitHub-Compatible Mermaid Diagrams

**Your Task: When asked to create a diagram, generate it using Mermaid syntax suitable for rendering directly in GitHub Flavored Markdown. Store the diagram in a new file at `docs/diagrams/<diagram-name>.md`. Adhere strictly to the following rules to ensure compatibility and readability.**

## I. Preliminary Step: Gather Context for the Diagram

Before generating any diagram, you **MUST** attempt to read and understand the content of the following project documents (if they exist and are relevant to the diagram request):

- `NOTES.md`
- `docs/PRD.md` (Product Requirements Document)
- `docs/TECH_STACK.md`
- `docs/openapi.yaml` (or other API specifications)
- Any other architecture or design documents in `docs/` related to the subject of the diagram.

This context is essential for creating an accurate and meaningful diagram.

## II. Core Diagramming Rules for GitHub Mermaid

1.  **Correct Fenced Code Block:**
    - Always start the Mermaid block with triple backticks and `mermaid`: `` ```mermaid ``.
    - Always end the block with triple backticks: ` ``` `.
    - **Example:**
        ```markdown
        ```mermaid
        graph TD
          A[Start] --> B{Decision};
          B -- Yes --> C[Action 1];
          B -- No --> D[Action 2];
        ```
        ```

2.  **Use Well-Supported Diagram Types:**
    GitHub generally has good support for these common types:
    - `graph` (Flowcharts - `TD` or `TB` is often best for readability)
    - `sequenceDiagram`
    - `classDiagram`
    - `stateDiagram-v2` (Prefer v2 for better features/rendering)
    - `erDiagram` (Entity Relationship)
    - `pie` (Pie charts)
    - `gantt` (Gantt charts)
    - `mindmap` (Basic indented structure only - **NO ICONS**)
    - *Avoid newer or less common diagram types, as GitHub's Mermaid version may not be the latest.*

3.  **General Syntax Best Practices:**
    - **Node IDs:** Use simple alphanumeric IDs (e.g., `node1`, `processA`). Avoid spaces or special characters in IDs.
    - **Node/Actor/State Labels (CRITICAL):** **ALWAYS use double quotes (`"..."`)** for labels, especially if they contain spaces, punctuation, special characters (like hyphens, periods, colons), or Mermaid keywords. This is the most common source of rendering errors.
        - **Correct:** `A["User Input: Text"] --> B["Validate Data (Step 1)"];`
        - **Incorrect (Potential Error):** `A[User Input: Text] --> B[Validate Data (Step 1)];`
    - **Arrows:** Use standard arrow types (`-->`, `---`, `==>`, `->>`, etc.).
    - **Comments:** Use `%%` for comments within the Mermaid code if needed (e.g., `%% This is a comment %%`).

4.  **`mindmap` Specifics for GitHub:**
    - GitHub **supports the basic `mindmap` structure** using indentation (spaces or tabs) to define hierarchy.
    - Each node/item **MUST** be on its own line with correct indentation relative to its parent.
    - GitHub **DOES NOT support** advanced `mindmap` features like `::icon()` syntax. Using icons **WILL CAUSE RENDERING ERRORS**.
    - Stick to plain text nodes for mind maps.
    - **Correct `mindmap` (GitHub Compatible):**
        ```mermaid
        mindmap
          Root
            Parent Node
              Child Item 1
              Child Item 2
        ```
    - **Incorrect `mindmap` (GitHub Incompatible - Uses Icons):**
        ```mermaid
        mindmap
          Root
            ::icon(fa fa-star) Parent Node
              ::icon(fa fa-one) Child Item 1
        ```

5.  **Layout Preference for Flowcharts (`graph`):**
    - For `graph` diagrams, prefer `TD` (Top Down) or `TB` (Top Bottom) for better readability within Markdown document flow. Example: `graph TD;`.

6.  **Styling: Let GitHub Handle It (CRITICAL):**
    - **DO NOT** attempt to set themes (e.g., `%%{init: {'theme': 'dark'}}%%`).
    - **DO NOT** try to apply custom styling using `classDef`, `style`, or inline CSS attributes *within the Mermaid code*.
    - GitHub **ignores** these directives and applies its own styling based on the user's current GitHub theme (light, dark, or dimmed). Your diagram will adapt automatically. Forcing themes or styles will likely be ignored or look inconsistent.

7.  **Keep Diagrams Focused and Manageable:**
    - Avoid overly complex diagrams with an excessive number of nodes, edges, or deep nesting in a single block. While GitHub can render complex diagrams, they might become hard to read or hit rendering performance limits.
    - If a concept is very complex, consider breaking it down into multiple, simpler, linked diagrams.

8.  **Verification (If Possible):**
    - If you have access to a GitHub Markdown preview environment, use it to test your Mermaid syntax. This is the best way to catch errors.

9.  **Note on Automated Edits:**
    - Be aware that automated code editing tools may sometimes struggle with precise changes within Mermaid blocks, especially with syntax sensitive to indentation (like mindmaps) or quoting.
    - **Always carefully review any automated edits** made to Mermaid blocks. If errors occur or the diagram doesn't render as expected, manual correction might be required.

## III. Output File

- Place the generated Mermaid diagram (within its fenced code block) into a new Markdown file.
- Save the file to `docs/diagrams/<diagram-name>.md`, where `<diagram-name>` is a descriptive name for the diagram (e.g., `user-authentication-flow.md`, `database-schema.md`).

**REMEMBER: Your goal is to create a clear, accurate Mermaid diagram in `docs/diagrams/<diagram-name>.md` that renders correctly on GitHub. Key success factors are: using ` ```mermaid `, quoting all labels meticulously (`"Like This!"`), avoiding themes/custom styles, and for mindmaps, using only basic indentation (NO icons). Always gather context from project docs first.**
</file>

<file path=".cursor/rules/docs-prd.mdc">
---
description:
globs:
alwaysApply: false
---
# Generate and Maintain `docs/PRD.md` (Product Requirements Document)

**Your Core Task: Create or update the `docs/PRD.md` file. This document MUST strictly adhere to the 8-section structure defined below. Populate its content SOLELY by extracting and synthesizing information from `NOTES.md` (or a user-provided equivalent source). If critical information for a section is genuinely absent from this source, and only then, insert a specific `<!-- TODO: ... -->` comment.**

Your entire output for this rule invocation **MUST BE ONLY** the complete, raw Markdown content of the `docs/PRD.md` file.

## I. Strict Operational Constraints (MANDATORY)

- **Permitted File Operations:**
    - **Read-Only:** `NOTES.md` (or the specified primary input document if different).
    - **Read/Write:** `docs/PRD.md` (this is the only file you will modify or create).
    - **No Other Files:** Do not access, read, or write any other files in the project.
- **Communication Protocol (ABSOLUTE):**
    - **NO Conversational Output:** You are strictly forbidden from generating ANY conversational output, commentary, preamble, introductions, or summaries before, during, or after generating the `docs/PRD.md` content.
    - **Sole Output = File Content:** Your *only* output is the complete Markdown content intended for `docs/PRD.md`.
- **User Interaction (Handled via TODOs in `docs/PRD.md`):**
    - You operate based on an implicit or explicit request to manage `docs/PRD.md`.
    - You do not converse directly with the user during this process.
    - Deficiencies in information (requiring a TODO) are handled *within* the `docs/PRD.md` file itself, as per the TODO logic below.

## II. The Blueprint: `docs/PRD.md` Structure & Content Source

The `docs/PRD.md` file **MUST** be organized into the following eight sections. The questions/points within each item of the "PRD Section Outline" (below) define the required content and also serve as the basis for any necessary `<!-- TODO: ... -->` comments if information is missing from the `NOTES.md` (or equivalent designated source document).

### PRD Section Outline (Mandatory Structure for `docs/PRD.md`)

1.  **`## 1. Core Functionality & Purpose`**
    - What is the primary problem this product/feature solves for the end-user?
    - What is the core functionality required to address this problem?

2.  **`## 2. Key Goals & Scope`**
    - What are the critical objectives for this product/feature (e.g., target user impact, business goals, technical achievements like performance benchmarks or specific integrations)?
    - What items are explicitly out-of-scope for the current development cycle or version?

3.  **`## 3. User Interaction & Design Insights`**
    - Who is the primary user type (e.g., API consumer, web application user, internal admin)?
    - Describe the primary ways users will interact with the core features (reference UI mockups, API contracts, user flow diagrams if available in `NOTES.md` or linked external resources).

4.  **`## 4. Essential Features & Implementation Highlights`**
    - List the absolute must-have functionalities for the initial version/MVP.
    - Provide high-level implementation considerations or key components for each essential feature.

5.  **`## 5. Acceptance Criteria & Definition of "Done"`**
    - For each key feature or user story, what are the specific, measurable, achievable, relevant, and time-bound (SMART) conditions that must be met for it to be considered "done"?
    - How will successful completion be verified (e.g., specific tests, user validation scenarios)?

6.  **`## 6. Key Requirements & Constraints`**
    - List any non-negotiable technical requirements (e.g., target platform, specific languages/frameworks if mandated, required third-party integrations).
    - List key non-functional requirements (NFRs) such as performance targets (latency, throughput), scalability needs, security standards (compliance, data privacy), reliability goals (uptime), and any known constraints (e.g., infrastructure limitations, budget, timelines).

7.  **`## 7. Success Metrics`**
    - How will the success of this product/feature be measured post-deployment from a user and business perspective (e.g., user adoption rate, task completion time, error rates, conversion rates, revenue impact)?
    - (Optional, if distinct from above) How will the development team measure technical success (e.g., system stability, maintainability, code quality metrics)?

8.  **`## 8. Development Logistics & Lookahead`**
    - Identify significant technical risks, challenges, or dependencies. Include initial thoughts on mitigation strategies.
    - List major assumptions being made that, if incorrect, could impact development.
    - Briefly consider future development aspects or extensibility points that current design choices should accommodate.

## III. Workflow for Generating/Updating `docs/PRD.md`

1.  **Access Source Document (`NOTES.md` or equivalent):**
    - Read the content of `NOTES.md` (or the user-specified primary input document).
    - *This is your ONLY source of information for populating `docs/PRD.md`.* If this document is missing or sparse, the `PRD.md` will reflect that (potentially with many TODOs).
    - Read the current content of `docs/PRD.md` if it already exists (for context during updates).

2.  **Manage `docs/PRD.md` Content:**
    - **Initialization:** If `docs/PRD.md` does not exist, create it. Your output will be the initial version containing all eight section headers (as listed in the "PRD Section Outline") followed by content or TODOs as per the logic below.
    - **Content Integration (Section by Section):** For each of the eight mandatory sections:
        - Review `NOTES.md` (and `docs/PRD.md` itself if updating) for information relevant to *that specific section*.
        - Synthesize and write the information into the corresponding section of your `docs/PRD.md` output. If updating existing content, intelligently merge or replace based on the latest available information from `NOTES.md`. Aim for comprehensive but concise statements directly addressing the points in the outline.

3.  **Identify Gaps & Insert TODOs (Strict Logic):**
    - After attempting to populate a section using **only `NOTES.md` (or the specified equivalent source)**:
        - A `<!-- TODO: ... -->` comment **MUST ONLY** be inserted if the section in your generated `docs/PRD.md` remains **genuinely empty** OR contains only placeholder text (e.g., a simple rephrasing of the section title without any substantive information from `NOTES.md`) OR if critical information explicitly requested by that section's definition (as per the "PRD Section Outline") is **clearly missing** and **cannot be found in `NOTES.md`**.
        - **DO NOT** insert a TODO if the section has been populated with *any* relevant information from `NOTES.md`, even if that information could theoretically be more detailed. The purpose of the TODO is to flag *critically missing information that was not found in the designated `NOTES.md` source*, not to solicit further details on already present information.
        - **TODO Comment Format:** `<!-- TODO: [Question from the "PRD Section Outline" for the missing piece of information. Be specific to the context if possible.] -->`
            - Example for section 1 if core problem isn't in `NOTES.md`: `<!-- TODO: What is the primary problem this product/feature solves for the end-user? (Expected in NOTES.md) -->`
            - Example for section 5 if acceptance criteria are absent: `<!-- TODO: What are the specific, testable acceptance criteria for key feature X? (Expected in NOTES.md) -->`

4.  **Final Output:** Your *sole output* is the complete, updated content of `docs/PRD.md`.

**REMEMBER: Your output MUST ONLY be the full Markdown content for `docs/PRD.md`. Populate it strictly from `NOTES.md` (or specified primary source). Follow the 8-section structure. Insert `<!-- TODO: ... -->` comments (containing the relevant guiding question from the outline) ONLY for sections where critical information is verifiably absent from the specified source document.**
</file>

<file path=".cursor/rules/docs-structure.mdc">
---
description:
globs:
alwaysApply: false
---
# Generate `docs/STRUCTURE.md` for Project File Organization

**Your task: Create a new file at `docs/STRUCTURE.md` that outlines the recommended file and directory structure for this project, along with guiding principles.**

Follow these steps:

1.  **Analyze Documentation:**
    - Thoroughly review all files within the `docs/` directory (e.g., `README.md`, `CONTRIBUTING.md`, architecture documents).
    - Look for any existing descriptions, guidelines, or discussions related to the project's intended file structure, module organization, or component layout.

2.  **Define Structure & Best Practices:**
    - Based on your findings (and general best practices if no specific information is available), define a clear and logical file/directory structure for the project.
    - Consider common conventions for the project's language(s) and framework(s).
    - Outline key top-level directories (e.g., `src/`, `tests/`, `docs/`, `scripts/`, `config/`) and their purposes.
    - Suggest organization for common components like source code, tests, utilities, configurations, etc.
    - Include any specific naming conventions if identified or appropriate.

3.  **Create `docs/STRUCTURE.md`:**
    - Write the content for the `docs/STRUCTURE.md` file.
    - The document should be well-organized, using Markdown for clarity (headings, lists, code blocks for examples if useful).
    - It should clearly explain the purpose of major directories and provide guidance on where to place new files or modules.

**Output:**

Your primary output is the complete content for the new `docs/STRUCTURE.md` file.

**REMEMBER: The goal is to produce a `docs/STRUCTURE.md` file that serves as a practical guide for organizing files and directories within this project, promoting consistency and maintainability.**
</file>

<file path=".cursor/rules/docs-tech-stack.mdc">
---
description:
globs:
alwaysApply: false
---
# Generate and Maintain `docs/TECH_STACK.md`

**Your Core Task: Create or update the `docs/TECH_STACK.md` file. This document MUST strictly adhere to the 10-section structure defined below. Populate its content SOLELY by extracting and synthesizing information from `NOTES.md`, `docs/PRD.md`, and `docs/openapi.yaml` (or equivalent API specifications). If critical information for a section is genuinely absent from these sources, and only then, insert a specific `<!-- TODO: ... -->` comment.**

Your entire output for this rule invocation **MUST BE ONLY** the complete, raw Markdown content of the `docs/TECH_STACK.md` file.

## I. Strict Operational Constraints (MANDATORY)

- **Permitted File Operations:**
    - **Read-Only:** `NOTES.md`, `docs/PRD.md`, `docs/openapi.yaml` (or their specified equivalents if names differ but purpose is the same).
    - **Read/Write:** `docs/TECH_STACK.md` (this is the only file you will modify or create).
    - **No Other Files:** Do not access, read, or write any other files in the project.
- **Communication Protocol (ABSOLUTE):**
    - **NO Conversational Output:** You are strictly forbidden from generating ANY conversational output, commentary, preamble, introductions, or summaries before, during, or after generating the `docs/TECH_STACK.md` content.
    - **Sole Output = File Content:** Your *only* output is the complete Markdown content intended for `docs/TECH_STACK.md`.
- **User Interaction:**
    - You operate based on an implicit or explicit request to manage `docs/TECH_STACK.md`.
    - You do not converse directly with the user during this process.
    - Deficiencies in information (requiring a TODO) are handled *within* the `docs/TECH_STACK.md` file itself, as per the TODO logic below.

## II. The Blueprint: `docs/TECH_STACK.md` Structure & Content Source

The `docs/TECH_STACK.md` file **MUST** be organized into the following ten sections. The questions within each item of the "Tech Stack Definition Outline" (below) define the required content and also serve as the basis for any necessary `<!-- TODO: ... -->` comments if information is missing from source documents.

### Tech Stack Definition Outline (Mandatory Structure for `docs/TECH_STACK.md`)

1.  **`## 1. Project Overview & Goals`**
    - *(Informed by `docs/PRD.md`)* Briefly, what is the project this tech stack is for?
    - *(Informed by `docs/PRD.md`)* What are the primary goals influencing technology choices (e.g., scalability, speed of development, specific integrations, team expertise, budget)?

2.  **`## 2. Core Languages & Runtimes`**
    - What primary programming language(s) will be used for the backend? Specify version(s) if critical. Why this choice?
    - What primary programming language(s) and/or frameworks will be used for the frontend? Specify version(s) if critical. Why this choice?
    - Are there specific runtime environments required (e.g., Node.js version, Python version, JVM version, .NET version)?

3.  **`## 3. Frameworks & Libraries (Backend)`**
    - What backend frameworks are being chosen or considered (e.g., Django, Ruby on Rails, Spring Boot, Express.js, NestJS, ASP.NET Core)? Justify the choice.
    - List key libraries essential for the backend (e.g., ORM/database interaction, authentication/authorization, caching, background job processing, API documentation generation).

4.  **`## 4. Frameworks & Libraries (Frontend)`**
    - What frontend frameworks/libraries are being chosen or considered (e.g., React, Angular, Vue, Svelte, Blazor)? Justify the choice.
    - List key UI component libraries (e.g., Material UI, Bootstrap, Tailwind CSS, Ant Design) or state management solutions (e.g., Redux, Zustand, Pinia, NgRx) to be used.

5.  **`## 5. Database & Data Storage`**
    - *(Consider data types/relationships in `docs/PRD.md`)* What type of database is required (e.g., Relational/SQL, NoSQL Document, NoSQL Key-Value, Graph, Time Series)? Why?
    - Specify the chosen database system(s) (e.g., PostgreSQL, MySQL, MongoDB, Cassandra, Neo4j, InfluxDB).
    - Are other data storage solutions needed (e.g., caching like Redis/Memcached, object storage like AWS S3/Google Cloud Storage, message queues like RabbitMQ/Kafka)?

6.  **`## 6. Infrastructure & Deployment`**
    - Where will the application be hosted (e.g., AWS, Azure, GCP, DigitalOcean, Vercel, Netlify, on-premise)?
    - What specific services will be used (e.g., EC2, Lambda, Azure App Service, Google Kubernetes Engine)?
    - What containerization technologies will be used (e.g., Docker, Podman)? Orchestration (e.g., Kubernetes, Docker Swarm)?
    - What CI/CD tools and processes are planned (e.g., Jenkins, GitLab CI, GitHub Actions, CircleCI)?

7.  **`## 7. APIs & Integrations`**
    - *(Reference `docs/PRD.md` and `docs/openapi.yaml`)* Will the project expose its own APIs? If so, what style (e.g., REST, GraphQL, gRPC, WebSockets)?
    - *(Reference `docs/PRD.md`)* What critical third-party services or APIs will be integrated (e.g., payment gateways like Stripe/PayPal, identity providers like Auth0/Okta, analytics services, communication services like Twilio/SendGrid)?

8.  **`## 8. Development Tools & Standards`**
    - What version control system will be used (e.g., Git)? Where will repositories be hosted (e.g., GitHub, GitLab, Bitbucket)?
    - Are there specific IDEs, linters (e.g., ESLint, Pylint), or code formatting standards (e.g., Prettier, Black)?
    - *(Reference `docs/PRD.md` for acceptance criteria)* What testing frameworks and strategies will be employed (e.g., Jest, PyTest, JUnit, Cypress, Selenium; unit, integration, E2E testing)?

9.  **`## 9. Security Considerations`**
    - *(Reference `docs/PRD.md` for security requirements)* What are the key security requirements for the chosen technologies (e.g., OWASP Top 10 mitigations)?
    - Are there specific libraries, tools, or practices for security (e.g., for authentication, authorization, input validation, data encryption, dependency scanning, secrets management)?

10. **`## 10. Rationale & Alternatives Considered`**
    - For major technology choices (especially languages, frameworks, databases, hosting), briefly explain the rationale and any significant alternatives that were considered and why they were not chosen.

## III. Workflow for Generating/Updating `docs/TECH_STACK.md`

1.  **Access Source Documents:**
    - Read the content of `NOTES.md` (if provided as an input or found).
    - Read the content of `docs/PRD.md`.
    - Read the content of `docs/openapi.yaml` (or equivalent API spec) if it exists.
    - *These are your ONLY sources of information for populating `docs/TECH_STACK.md`.* If these documents are missing or sparse, the `TECH_STACK.md` will reflect that (potentially with TODOs).
    - Read the current content of `docs/TECH_STACK.md` if it already exists.

2.  **Manage `docs/TECH_STACK.md` Content:**
    - **Initialization:** If `docs/TECH_STACK.md` does not exist, create it. Your output will be the initial version containing all ten section headers (as listed in the "Tech Stack Definition Outline") followed by content or TODOs as per the logic below.
    - **Content Integration (Section by Section):** For each of the ten mandatory sections:
        - Review `NOTES.md`, `docs/PRD.md`, and `docs/openapi.yaml` (and `TECH_STACK.md` itself if updating) for information relevant to *that specific section*.
        - Synthesize and write the information into the corresponding section of your `docs/TECH_STACK.md` output. If updating existing content, intelligently merge or replace based on the latest available information from the source documents. Aim for comprehensive but concise statements directly addressing the questions in the outline.

3.  **Identify Gaps & Insert TODOs (Strict Logic):**
    - After attempting to populate a section using **all three source documents** (`NOTES.md`, `docs/PRD.md`, `docs/openapi.yaml`):
        - A `<!-- TODO: ... -->` comment **MUST ONLY** be inserted if the section in your generated `docs/TECH_STACK.md` remains **genuinely empty** OR contains only placeholder text (e.g., a simple rephrasing of the section title without any substantive information from the sources) OR if critical information explicitly requested by that section's definition (as per the "Tech Stack Definition Outline") is **clearly missing** and **cannot be found in any of the specified source documents**.
        - **DO NOT** insert a TODO if the section has been populated with *any* relevant information from the source files, even if that information could theoretically be more detailed or elaborated upon. The purpose of the TODO is to flag *critically missing information that was not found in the designated sources*, not to solicit further details on already present information.
        - **TODO Comment Format:** `<!-- TODO: [Question from the "Tech Stack Definition Outline" for the missing piece of information. Include examples if helpful.] -->`
            - Example for section 1 if project goals are missing from PRD: `<!-- TODO: What are the primary goals influencing technology choices (e.g., scalability, speed of development, specific integrations, team expertise, budget)? (This should be in docs/PRD.md) -->`
            - Example for section 2 if backend language choice isn't found: `<!-- TODO: What primary programming language(s) will be used for the backend? Specify version(s) if critical. Why this choice? -->`

4.  **Final Output:** Your *sole output* is the complete, updated content of `docs/TECH_STACK.md`.

**REMEMBER: Your output MUST ONLY be the full Markdown content for `docs/TECH_STACK.md`. Populate it strictly from `NOTES.md`, `docs/PRD.md`, and `docs/openapi.yaml`. Follow the 10-section structure. Insert `<!-- TODO: ... -->` comments (containing the guiding question from the outline) ONLY for sections where critical information is verifiably absent from all specified source documents.**
</file>

<file path=".cursor/rules/pnpm-fixes.mdc">
---
description:
globs:
alwaysApply: false
---
# Resolve PNPM Build Script Issues for Trusted Dependencies

**Your task: When `pnpm` blocks essential build scripts from trusted dependencies during installation or execution, apply one of the following solutions.**

This typically occurs because `pnpm` defaults to not running build scripts for security reasons. For well-known and trusted libraries, you may need to explicitly allow them.

## Solutions:

1.  **Allow Specific Dependencies via `package.json`:**
    To permanently allow build scripts for specific, trusted dependencies, modify your `package.json` file by adding or updating the `pnpm.onlyBuiltDependencies` field. This is the preferred method for ongoing project configurations.

    **Example (`package.json`):**
    ```json
    {
      // ... other package.json content
      "pnpm": {
        "onlyBuiltDependencies": [
          "sqlite3",
          "another-trusted-dependency-requiring-build"
        ]
      }
      // ... rest of package.json content
    }
    ```
    *This tells pnpm to allow build scripts specifically for `sqlite3` and `another-trusted-dependency-requiring-build`.*

2.  **Temporarily Allow All Build Scripts During Installation (Use with Caution):**
    For a one-time installation where you need to allow all build scripts (e.g., when installing many new, trusted dependencies that require building), you can use the `--dangerously-allow-all-builds` flag with the install command.

    **Command:**
    ```bash
    pnpm install --dangerously-allow-all-builds
    ```
    - **Warning:** This flag bypasses a security feature. Use it only when you trust all dependencies being installed or built. Prefer Solution 1 for specific, ongoing needs.*

**REMEMBER: Apply these solutions when `pnpm`'s security measures prevent necessary build scripts from *trusted* dependencies from running. Prioritize the `package.json` configuration for persistent allowances.**
</file>

<file path=".cursor/rules/project-todos-next.mdc">
---
description:
globs:
alwaysApply: false
---
# Identify and Report TODO Items from Documentation (`docs/`)

**Your task: Scan all files within the `docs/` directory to find comments or notes marked as "TODO", and then list the most pressing or contextually relevant ones.**

Follow this process:

1.  **Scan Documentation for TODOs:**
    - Systematically search the content of every file located within the `docs/` directory (and its subdirectories).
    - Look for lines or comments explicitly containing the keyword "TODO" (case-insensitive, if appropriate for the tool being used for searching).

2.  **Extract and Contextualize TODOs:**
    - For each identified TODO, extract the full text of the TODO comment.
    - Note the file path and line number where the TODO was found.

3.  **Filter and Report Relevant TODOs:**
    - From the list of all found TODOs, select those that appear most pressing, actionable, or relevant to the current project goals or ongoing tasks.
    - Consider factors like urgency implied by the TODO text, relation to known issues, or impact on project progress.
    - Present the selected TODOs in a clear, itemized list.

**Output Format:**

Provide a list of the identified TODOs, formatted as follows for each:

*   **TODO:** [Full text of the TODO comment]
    *   **Location:** `docs/path/to/file.md:line_number`
    *   **(Optional) Priority/Relevance:** [Brief note on why it's considered pressing, if applicable]

**REMEMBER: Your goal is to surface actionable TODO items embedded within project documentation that require attention, helping to ensure that planned work or notes for improvement are not overlooked.**
</file>

<file path=".cursor/rules/react-rules.mdc">
---
description:
globs:
alwaysApply: false
---

# React: Apply `useEffect` Dependency Best Practices

**Your task: When using `useEffect` in React components, ensure dependency arrays are correctly managed to prevent infinite loops and optimize performance.**

## Guiding Principle

To avoid infinite loops with `useEffect`, **DO NOT** include functions in the dependency array if they are redefined on every render. For effects that should only run on component mount and unmount, use an empty dependency array (`[]`).

If a function *must* be a dependency because its identity or behavior changes and should trigger the effect, **ALWAYS** memoize it using `useCallback`.

### Examples

**Incorrect (Potential Infinite Loop):**
```tsx
useEffect(() => {
  fetchNotes();
}, [fetchNotes]); // PROBLEM: fetchNotes is typically re-created every render, causing an infinite loop if not memoized.
```

**Correct (Runs Only on Mount):**
```tsx
useEffect(() => {
  fetchNotes();
}, []); // INTENT: Effect runs once after initial render.
```

**Correct (Function as a Memoized Dependency):**
```tsx
const fetchNotes = useCallback(() => { 
  // ... logic for fetching notes ...
}, []); // useCallback memoizes fetchNotes, its identity is stable unless its own dependencies change.

useEffect(() => {
  fetchNotes();
}, [fetchNotes]); // CORRECT: Effect re-runs only if fetchNotes (due to its own useCallback dependencies) changes.
```

**REMEMBER: Always critically evaluate `useEffect` dependencies. Use an empty array `[]` for mount-only effects. If a function is a dependency, ensure it's stable by wrapping it in `useCallback`.**
</file>

<file path=".cursor/rules/scripts-create.mdc">
---
description:
globs:
alwaysApply: false
---
# Create Executable TypeScript Scripts with Bun

**Your task: Create new, executable TypeScript scripts within a `scripts` directory, runnable with `bun`.**

Follow these steps precisely:

1.  **Ensure `scripts` Directory:** If it doesn't already exist, create a `scripts/` directory in the project root.
2.  **Create Script File:** Inside the `scripts/` directory, create your new TypeScript file (e.g., `scripts/my-new-script.ts`).
3.  **Add Bun Shebang:** Make the first line of your script: `#!/usr/bin/env bun`.
4.  **Write TypeScript Code:** Implement the script's logic in TypeScript. Ensure the code is compatible with Node.js execution environments where possible, even though `bun` is the primary runner.
5.  **Make Executable:** After saving the script, make it executable by running the command: `chmod +x scripts/your-script-name.ts` (replace `your-script-name.ts` with the actual filename).

**REMEMBER: The final script must be a TypeScript file located in the `scripts/` directory, start with the `#!/usr/bin/env bun` shebang, and be executable (`chmod +x`).**
</file>

<file path=".cursor/rules/task-next.mdc">
---
description:
globs:
alwaysApply: false
---
# Propose Next Development Task

**Your primary task: Identify and recommend the most logical next development task based on the current project state and goals.**

To achieve this, follow these thought processes:

1.  **Understand Context:**
    - Review relevant project documentation (e.g., files in `docs/`, `README.md`).
    - Analyze current project requirements. If a `.cursor/rules/req-task.mdc` (or similar) file exists and is relevant, consult it for specific task requirements or context.
    - Consider the most recent actions, discussions, or completed work in the project.

2.  **Identify Potential Tasks:**
    - Based on your understanding, brainstorm a list of 3-5 potential next tasks that would advance the project.

3.  **Evaluate and Recommend:**
    - From your list, select the single most logical and impactful task to undertake next.
    - Clearly state your recommendation.
    - Provide concise reasoning for your choice, explaining why it's the most appropriate next step (e.g., dependency for other tasks, highest priority, unblocks other work).

**Output Expectation:**

Your response should clearly state:
1.  **Recommended Next Task:** [Describe the task]
2.  **Reasoning:** [Explain why this task is recommended]
3.  **(Optional) Alternative Tasks:** [Briefly list 1-2 other potential tasks considered]

**REMEMBER: Your goal is to provide a well-reasoned recommendation for the single most impactful next development task.**
</file>

<file path=".cursor/rules/zen-coding.mdc">
---
description:
globs:
alwaysApply: false
---
# Adhere to the Zen of Coding Principles

**Your primary goal: Internalize and apply the following principles to guide your coding decisions, striving for code that is clear, maintainable, and robust.**

These principles should influence how you approach problem-solving, structure your code, and handle complexities.

## Guiding Principles:

- Beautiful is better than ugly.
- Explicit is better than implicit.
- Simple is better than complex.
- Complex is better than complicated.
- Flat is better than nested.
- Sparse is better than dense.
- Readability counts.
- Special cases aren't special enough to break the rules.
- Although practicality beats purity.
- Errors should never pass silently.
- Unless explicitly silenced.
- In the face of ambiguity, refuse the temptation to guess.
- There should be one-- and preferably only one --obvious way to do it.
    - (Though that way may not be obvious at first.)
- Now is better than never.
- Although never is often better than *right* now.
- If the implementation is hard to explain, it's a bad idea.
- If the implementation is easy to explain, it may be a good idea.

**REMEMBER: Let these tenets guide you towards writing code that is not only functional but also elegant and understandable.**
</file>

<file path=".cursor/rules/cli-github-search.mdc">
---
description: Search GitHub for examples
globs:
alwaysApply: false
---
# How to Use the `ghx` CLI for GitHub Code Search

**Your task: When you need to find code examples, files, or specific patterns on GitHub, use the `ghx` command-line tool. This document provides its usage instructions and examples.**

## `ghx` Command Reference

Find examples of files, code, etc on GitHub

Usage: ghx [options]

Commands:
  ghx config      Manage configuration settings
  ghx [query]     Search GitHub Code                                   [default]

Positionals:
  query  Search query                                                   [string]

Options:
      --version       Show version number                              [boolean]
  -h, --help          Show help                                        [boolean]
  -p, --pipe          Output results directly to stdout                [boolean]
  -d, --debug         Output code fence contents for testing           [boolean]
  -L, --limit         Maximum number of results to fetch  [number] [default: 50]
  -f, --max-filename  Maximum length of generated filenames
                                                          [number] [default: 50]
  -c, --context       Number of context lines around matches
                                                          [number] [default: 20]
  -r, --repo          Search in a specific repository (owner/repo)      [string]
  -P, --path          Search in a specific path                         [string]
  -l, --language      Search for files in a specific language           [string]
  -e, --extension     Search for files with a specific extension        [string]
  -n, --filename      Search for files with a specific name             [string]
  -s, --size          Search for files of a specific size               [string]
  -F, --fork          Include or exclude forked repositories           [boolean]

Examples:
  ghx 'useState'                            Search for 'useState' across all ind
                                            exed code on GitHub
  ghx --repo facebook/react "useState"      Search for 'useState' in the faceboo
                                            k/react repository
  ghx -l typescript -e tsx "useState"       Search for 'useState' in TypeScript
                                            files with the .tsx extension
  ghx -n package.json "dependencies"        Search for 'dependencies' specifical
                                            ly within package.json files
  ghx -P src/components "Button"            Search for 'Button' within the src/c
                                            omponents path
  ghx -s '">10000" -l go "package main"     Search for 'package main' in Go file
                                            s larger than 10KB
  ghx "async function" -l typescript        Search for the exact phrase 'async f
                                            unction' in TypeScript files
  ghx "my search terms" --pipe > results.m  Search and pipe the results directly
  d                                          to a markdown file
  ghx -L 100 -c 30 "complex query"          Fetch up to 100 results with 30 line
                                            s of context per match
  ghx -l typescript "import test"           Search for lines containing both 'im
                                            port' AND 'test' in TypeScript files
  ghx -l javascript "const OR let"          Search for lines containing either '
                                            const' OR 'let' in JavaScript files
  ghx -l css "color NOT background-color"   Search for lines containing 'color'
                                            BUT NOT 'background-color' in CSS fi
                                            les

**REMEMBER: Refer to these instructions and examples when constructing `ghx` commands to perform targeted and effective GitHub code searches.**
</file>

<file path=".cursor/rules/cli-worktree.mdc">
---
description:
globs:
alwaysApply: false
---
# Using `wt` CLI for Git Worktree Management

**Your Task: When managing Git worktrees, especially for integration with the Cursor editor, use the `wt` command-line tool. This document provides its usage instructions, examples, and important operational guidelines.**

**Initial Setup Note:** If `wt` commands fail, the tool might need to be installed globally. Refer to `https://www.npmjs.com/package/@johnlindquist/worktree` for installation instructions (e.g., `pnpm add -g @johnlindquist/worktree`).

## Operational Guidelines for AI:

- **Default Action:** When the user expresses intent related to a new worktree (e.g., "create a worktree for branch X", "start work on Y"), assume they want to create a *new* worktree using `wt new [branchName]` unless they explicitly specify another `wt` command (like `list`, `remove`, etc.).
- **Post-Creation Pause:** After successfully creating a new worktree using `wt new`, pause and await the user's next instruction. Do not automatically proceed with other actions unless explicitly part of a larger, pre-defined plan.
- **Dependency Installation (CRITICAL DEFAULT):** When executing `wt new`, **ALWAYS append `-i pnpm`** to the command to ensure dependencies are installed using `pnpm` by default. For example: `wt new feature/login -i pnpm`.

## `wt` Command Reference

Usage: `wt [options] [command]`

Description: Manage git worktrees and open them in the Cursor editor.

### Options:
  `-V, --version`                       Output the version number
  `-h, --help`                          Display help for command

### Commands:

  `new [options] [branchName]`
    Create a new worktree for the specified branch.
    Installs dependencies if specified (see `-i` option under command-specific options, and AI default above).
    Opens in editor.

  `list | ls`
    List all existing worktrees for this repository.

  `remove | rm [options] [pathOrBranch]`
    Remove a specified worktree. Cleans up `.git/worktrees` references.

  `merge [options] <branchName>`
    Commit changes in the target branch and merge them into the current branch, then remove the branch/worktree.

  `purge`
    Safely remove all worktrees except for the main branch, with confirmation.

  `pr [options] <prNumber>`
    Fetch the branch for a given GitHub PR number and create a worktree.

  `config`
    Manage CLI configuration settings.

  `help [command]`
    Display help for a specific command.

### Examples for `wt new`:

- `wt new feature/login -i pnpm` (AI Default: creates worktree, installs with pnpm)
- `wt new feature/chat --checkout -i pnpm` (AI Default: creates, checks out, installs with pnpm)
- `wt new feature/auth -p ./auth-worktree -i pnpm` (AI Default: creates at path, installs with pnpm)
- `wt new feature/vscode -e code -i pnpm` (AI Default: creates, opens in VSCode, installs with pnpm)

*(Note: The examples above have been updated to include the AI's default `-i pnpm` behavior.)*

**REMEMBER: Use `wt` for Git worktree operations. Always assume `wt new` is the primary intent for new worktree requests and ALWAYS include `-i pnpm` with `wt new` for dependency installation.**
</file>

<file path=".cursor/rules/docs-sync.mdc">
---
description:
globs:
alwaysApply: false
---
# Synchronize Project Documentation with Recent Changes

**Your task: Review and update all project documentation within the `docs/` directory to ensure it accurately reflects the latest project state, recent code changes, and completed tasks.**

Follow this process:

1.  **Identify Recent Project Developments:**
    - Review information about recently completed tasks (e.g., from `docs/tasks/`, task management systems if accessible, or summarized in recent discussions).
    - Examine recent commit history for significant changes to code, features, or configurations.
    - Consider any new features added or bugs fixed that might impact documentation.

2.  **Thoroughly Review Documentation:**
    - Read all files located within the `docs/` directory and its subdirectories.
    - Pay close attention to READMEs, guides, architectural documents, and usage instructions.

3.  **Identify and Update Discrepancies:**
    - Compare the current documentation against the recent developments identified in Step 1.
    - Identify any sections, instructions, diagrams, or examples in `docs/*` that are now out-of-date, inaccurate, or incomplete.
    - Make the necessary modifications to these files to reflect the current state of the project accurately.

**Output:**

- Apply edits directly to the relevant files within the `docs/` directory.
- If significant changes are made, briefly summarize which documents were updated and the nature of the changes (e.g., "Updated `docs/API.md` to reflect new endpoint X and parameter Y.").

**REMEMBER: Your goal is to ensure all documentation in `docs/` is consistent, accurate, and fully synchronized with the project's latest developments. Outdated documentation can mislead users and developers.**
</file>

<file path=".cursor/rules/project-update-rules.mdc">
---
description:
globs:
alwaysApply: false
---
# Update Project Context File: .cursor/rules/_project.mdc

**Your primary task: Meticulously update the `./cursor/rules/_project.mdc` file. Use ONLY information found within the current project's files and documentation.**

## 1. Core Objective

Your goal is to populate the `./cursor/rules/_project.mdc` file. This file must accurately reflect the project's current configuration, technology stack, structure, standards, and tooling, serving as a central project reference.

## 2. Information Gathering (Strictly from Project Sources)

Follow these steps precisely to gather the necessary information:

### Step 1: Analyze Project Root & Configuration Files:
- List all files and directories in the project root.
- Identify and thoroughly read relevant configuration files. Examples include (but are not limited to):
    - Package management: `package.json`, `pnpm-lock.yaml`, `yarn.lock`, `pyproject.toml`, `poetry.lock`, `go.mod`, `pom.xml`, `build.gradle`, `Gemfile`, `composer.json`
    - Build tools & bundlers: `webpack.config.js`, `vite.config.js`, `rollup.config.js`, `tsconfig.json` (check for `compilerOptions`, `extends`, etc.)
    - Framework specific: `next.config.js`, `angular.json`, `vue.config.js`, `svelte.config.js`
    - Linters & formatters: `.eslintrc.js`, `.eslintignore`, `.prettierrc.js`, `.prettierignore`, `biome.json`, `ruff.toml`, `.editorconfig`
    - CI/CD: Files in `.github/workflows/`, `gitlab-ci.yml`, `Jenkinsfile`, `Dockerfile`
    - Testing: `jest.config.js`, `vitest.config.js`, `cypress.json`, `playwright.config.js`
- From these files, extract details about dependencies (and their versions), scripts (build, dev, test), language versions, tool configurations, and project settings.

### Step 2: Review Documentation and Key Code Directories:
- Thoroughly read all files located within the `docs/` directory (and its subdirectories). Prioritize `README.md`, `CONTRIBUTING.md`, architecture diagrams, technical specifications, or any documents describing the project's setup, conventions, and purpose.
- Examine the structure of main source directories (e.g., `src/`, `app/`, `lib/`, `packages/`) to understand module organization and identify key architectural patterns.

## 3. Updating `./cursor/rules/_project.mdc`

### Step 3: Populate the Template:
- Open the `./cursor/rules/_project.mdc` file for editing.
- Using the information gathered in Steps 1 and 2, meticulously fill in each section of the template provided below.
- **Crucial:** Adhere strictly to the template's structure and placeholders. Provide specific examples and details where requested (e.g., actual library names, version numbers, file paths).
- **Handling Missing Information:** If, after thorough investigation, specific information required by the template cannot be definitively found within the project's files or `docs/` directory, explicitly state `[Information not found in project context]` or `[N/A]` in the corresponding field. **ABSOLUTELY DO NOT invent, infer, or use external knowledge.** It is critical to indicate missing information rather than provide incorrect data.

## 4. Output Requirements

- Your SOLE output must be the fully updated content of the `./cursor/rules/_project.mdc` file.
- Ensure the output is valid Markdown.

---
**REMEMBER: Your objective is to produce an accurate, detailed, and comprehensive `./cursor/rules/_project.mdc` file. This file MUST be populated using *only* information extracted directly from THIS project's codebase and its documentation. Adhere strictly to the provided template. After updating the file with all available project-specific information (or marking details as 'not found'), your task is complete.**
---

## .cursor/rules/_project.mdc Template

Use this template as guide for creating the _project.mdc:

<template>
---
description:
globs:
alwaysApply: true
---

# [Project Name] - Project Rules

## 1. Project Overview

goal: [Concisely describe the main purpose and goal of the project. What problem does it solve? Source from README or project vision docs.]
type: [e.g., Web Application, CLI Tool, Library, Mobile App, Backend API, Monorepo. Determine from project structure and build files.]
key features:
- [core functionality or feature 1, derived from docs or main modules]
- [core functionality or feature 2, derived from docs or main modules]
- [core functionality or feature 3, derived from docs or main modules]

## 2. Technology Stack

language(s): [e.g., TypeScript 5.x (from tsconfig.json or package.json), Python 3.11 (from pyproject.toml or runtime checks), Go 1.23 (from go.mod), Java 17 (from pom.xml or build.gradle)]
framework(s): [e.g., Next.js 15 (App Router - check next.config.js, package.json), React 19 (package.json), FastAPI (pyproject.toml, main app file), Spring Boot (pom.xml), SvelteKit (svelte.config.js)]
ui library/styling: [e.g., Tailwind CSS v4 (tailwind.config.js, package.json), Shadcn UI (components.json, registry), Material UI (package.json), CSS Modules (file extensions, build config)]
database(s): [e.g., PostgreSQL via Prisma (schema.prisma, package.json), MongoDB (connection strings, package.json), Supabase (config, client usage), SQLite via Drizzle (drizzle.config.js, package.json)]
state management: [e.g., Zustand (package.json, store files), React Context (usage in components), Redux (package.json, store setup), Pinia (package.json, store setup), None (Local State - if no dedicated library found)]
api layer: [e.g., REST (via Next.js API Routes/FastAPI), GraphQL (schema files, Apollo/Relay packages), tRPC (router definitions, client setup)]
key libraries:
- [critical dependency 1 (e.g., `axios` for HTTP, `date-fns` for dates - from package.json/pyproject.toml etc.)]
- [critical dependency 2 (e.g., `zod` for validation, `pino` for logging)]

## 3. Project Structure

main source directory: [e.g., src/, app/, packages/ - identify primary code location]
core directories: [Verify existence and common usage patterns]
- components/: [e.g., Reusable UI elements - if applicable]
- lib/ or utils/: [e.g., Shared utility functions - if applicable]
- services/ or api/: [e.g., Business logic, API interactions - if applicable]
- types/ or interfaces/: [e.g., Shared type definitions - if applicable]
- db/ or prisma/ or drizzle/: [e.g., Database schema and access - if applicable]
- tests/ or __tests__/: [e.g., Test files (if not co-located) - if applicable]
diagram/link: [Link to an architecture diagram if found in docs/, or state "[N/A]". Do not generate one.]

## 4. Coding Standards & Conventions

language usage: [e.g., Prefer functional components (React), Use async/await (JS/TS), Strict TypeScript mode (tsconfig.json `strict: true`), Avoid `any` (TS lint rules). Source from linting configs, `CONTRIBUTING.md`.]
naming conventions:
- files/folders: [e.g., kebab-case, PascalCase - observe project files, check `CONTRIBUTING.md`]
- components: [e.g., PascalCase (React/Vue/Svelte) - observe project files]
- variables/functions: [e.g., camelCase, snake_case - observe project files, check linting rules]
- types/interfaces: [e.g., PascalCase, TPrefix or IPrefix - observe project files, check `CONTRIBUTING.md`]
code style/formatting: [e.g., Prettier (check .prettierrc, package.json scripts), ESLint (check .eslintrc, package.json scripts), Ruff (ruff.toml), Biome (biome.json) - mention config file if present.]
comments: [e.g., English only, JSDoc for public APIs, Minimal comments - check `CONTRIBUTING.md` or observe codebase patterns.]
imports: [e.g., Absolute paths (@/ or tsconfig paths), Relative paths, Grouped/Sorted (check lint rules like eslint-plugin-import) - check `CONTRIBUTING.md`, linting config.]

## 5. Key Principles & Best Practices

[Source these from `CONTRIBUTING.md`, `README.md`, or high-level design documents in `docs/`. If none explicitly stated, mark as `[No explicit principles documented]`. Examples:]
- [e.g., DRY (Don't Repeat Yourself)]
- [e.g., SOLID principles for OOP]
- [e.g., Test-Driven Development (TDD)]

## 6. Testing

framework: [e.g., Jest, Vitest, Pytest, Go testing, Cypress, Playwright - from package.json, config files like jest.config.js]
types: [e.g., Unit tests required for services (from `CONTRIBUTING.md`), Integration tests for API endpoints, E2E with Playwright/Cypress - from `CONTRIBUTING.md` or test file structure.]
location: [e.g., Co-located with source files (e.g., `*.test.ts`, `*.spec.ts`), Top-level `tests/` directory - observe project structure.]
coverage expectations: [e.g., Minimum 80% coverage (from CI config or `CONTRIBUTING.md`). If not found, state `[N/A]`.]

## 7. Tooling & Workflow

package manager: [e.g., pnpm (pnpm-lock.yaml), npm (package-lock.json), yarn (yarn.lock), bun (bun.lockb), poetry (poetry.lock), uv (uv.lock) - identify from lockfile or project setup docs.]
build command(s): [e.g., `pnpm build`, `npm run build`, `make build` - from `package.json` scripts, `Makefile`, etc.]
run command(s) (dev): [e.g., `pnpm dev`, `npm start`, `python main.py`, `go run ./cmd/...` - from `package.json` scripts, `Makefile`, `README.md`.]
version control: [e.g., Git. Check for Conventional Commits (commitlint.config.js, `CONTRIBUTING.md`), PRs to `main`/`master` branch (from `CONTRIBUTING.md` or repo settings if accessible).]
ci/cd: [e.g., GitHub Actions (check `.github/workflows/`), GitLab CI (`.gitlab-ci.yml`), Jenkins (`Jenkinsfile`) - specify main jobs like lint, test, build on PR.]
ide recommendations: [e.g., VS Code with specific extensions (check `.vscode/extensions.json`). If not found, state `[N/A]`.]

## 8. (Optional) Database / API Guidelines

[Source from `docs/database.md`, `docs/api_guidelines.md`, `CONTRIBUTING.md`, or inline comments in DB/API code. If none, state `[N/A]`.]
- [e.g., Use ORM methods only, No direct SQL unless approved]
- [e.g., RESTful principles for API design, specific error response format]
- [e.g., Guidelines for database migrations (e.g., use Alembic, Prisma Migrate)]

## 9. (Optional) Specific Feature Rules

[Source from dedicated docs for complex features (e.g., `docs/authentication.md`, `docs/i18n.md`). If none, state `[N/A]`.]
- [e.g., Authentication: JWT-based, specific token handling procedures]
- [e.g., Internationalization (i18n): Use i18next, key naming conventions]
- [e.g., State Management: Rules for creating new stores, selector patterns]

## 10. (Optional) Rule Referencing

[If this project uses other `.mdc` rule files, list them here. Check the `.cursor/rules/` directory. If none, state `[N/A]`.]
- [e.g.,
  - See [typescript.mdc](mdc:.cursor/rules/typescript.mdc) for detailed TS rules.
  - Follow guidelines in [auth.mdc](mdc:.cursor/rules/auth.mdc) for authentication.
  ]
</template>
</file>

<file path=".gitignore">
# SpecStory explanation file
.specstory/.what-is-this.md

# Logs
logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*
lerna-debug.log*
.pnpm-debug.log*

# Diagnostic reports (https://nodejs.org/api/report.html)
report.[0-9]*.[0-9]*.[0-9]*.[0-9]*.json

# Runtime data
pids
*.pid
*.seed
*.pid.lock

# Directory for instrumented libs generated by jscoverage/JSCover
lib-cov

# Coverage directory used by tools like istanbul
coverage
*.lcov

# nyc test coverage
.nyc_output

# Grunt intermediate storage (https://gruntjs.com/creating-plugins#storing-task-files)
.grunt

# Bower dependency directory (https://bower.io/)
bower_components

# node-waf configuration
.lock-wscript

# Compiled binary addons (https://nodejs.org/api/addons.html)
build/Release

# Dependency directories
node_modules/
jspm_packages/

# Snowpack dependency directory (https://snowpack.dev/)
web_modules/

# TypeScript cache
*.tsbuildinfo

# Optional npm cache directory
.npm

# Optional eslint cache
.eslintcache

# Optional stylelint cache
.stylelintcache

# Microbundle cache
.rpt2_cache/
.rts2_cache_cjs/
.rts2_cache_es/
.rts2_cache_umd/

# Optional REPL history
.node_repl_history

# Output of 'npm pack'
*.tgz

# Yarn Integrity file
.yarn-integrity

# dotenv environment variable files
.env
.env.development.local
.env.test.local
.env.production.local
.env.local

# parcel-bundler cache (https://parceljs.org/)
.cache
.parcel-cache

# Next.js build output
.next
out

# Nuxt.js build / generate output
.nuxt
dist

# Gatsby files
.cache/
# Comment in the public line in if your project uses Gatsby and not Next.js
# https://nextjs.org/blog/next-9-1#public-directory-support
# public

# vuepress build output
.vuepress/dist

# vuepress v2.x temp and cache directory
.temp
.cache

# vitepress build output
**/.vitepress/dist

# vitepress cache directory
**/.vitepress/cache

# Docusaurus cache and generated files
.docusaurus

# Serverless directories
.serverless/

# FuseBox cache
.fusebox/

# DynamoDB Local files
.dynamodb/

# TernJS port file
.tern-port

# Stores VSCode versions used for testing VSCode extensions
.vscode-test

# yarn v2
.yarn/cache
.yarn/unplugged
.yarn/build-state.yml
.yarn/install-state.gz
.pnp.*
tmp/
</file>

<file path="cli.js">
#!/usr/bin/env node

const https = require("https");
const fs = require("fs");
const path = require("path");
const os = require("os");

const DEFAULT_ORG = "johnlindquist";
const DEFAULT_REPO = "get-rules";
const RULES_PATH = ".cursor/rules";

// Parse optional org/repo argument
const userArg = process.argv[2];
let org = DEFAULT_ORG;
let repo = DEFAULT_REPO;
if (userArg && /^[^/]+\/[^/]+$/.test(userArg)) {
	[org, repo] = userArg.split("/");
	console.log(`Using custom repo: ${org}/${repo}`);
} else if (userArg) {
	console.warn(
		`Ignoring invalid argument '${userArg}'. Expected format: org/repo`,
	);
}

const GITHUB_API_URL = `https://api.github.com/repos/${org}/${repo}/contents/${RULES_PATH}`;
const DEST_DIR_NAME = RULES_PATH; // Relative to current working directory

// Helper function to make an HTTPS GET request and parse JSON response
function httpsGetJson(url) {
	return new Promise((resolve, reject) => {
		const options = {
			headers: {
				"User-Agent":
					"get-rules-npm-script/1.0.0 (github.com/johnlindquist/get-rules)",
			},
		};
		https
			.get(url, options, (res) => {
				if (res.statusCode < 200 || res.statusCode >= 300) {
					return reject(
						new Error(
							`GitHub API request failed: ${res.statusCode} for ${url}`,
						),
					);
				}
				let rawData = "";
				res.on("data", (chunk) => (rawData += chunk));
				res.on("end", () => {
					try {
						resolve(JSON.parse(rawData));
					} catch (e) {
						reject(new Error(`Failed to parse JSON from ${url}: ${e.message}`));
					}
				});
			})
			.on("error", (err) => {
				reject(new Error(`HTTPS request error for ${url}: ${err.message}`));
			});
	});
}

// Helper function to download a file
function downloadFile(fileUrl, destinationPath) {
	return new Promise((resolve, reject) => {
		const fileStream = fs.createWriteStream(destinationPath);
		const options = {
			headers: {
				"User-Agent":
					"get-rules-npm-script/1.0.0 (github.com/johnlindquist/get-rules)",
			},
		};
		https
			.get(fileUrl, options, (response) => {
				if (response.statusCode < 200 || response.statusCode >= 300) {
					fs.unlink(destinationPath, () => {}); // Clean up empty file on error
					return reject(
						new Error(
							`Failed to download ${fileUrl}. Status: ${response.statusCode}`,
						),
					);
				}
				response.pipe(fileStream);
				fileStream.on("finish", () => {
					fileStream.close(resolve);
				});
			})
			.on("error", (err) => {
				fs.unlink(destinationPath, () => {}); // Clean up if error occurs
				reject(new Error(`Error downloading ${fileUrl}: ${err.message}`));
			});
	});
}

async function main() {
	const absoluteDestDir = path.resolve(process.cwd(), DEST_DIR_NAME);
	console.log(`Attempting to install rules to ${absoluteDestDir}`);

	try {
		// 1. Ensure destination directory exists
		if (!fs.existsSync(absoluteDestDir)) {
			fs.mkdirSync(absoluteDestDir, { recursive: true });
			console.log(`Created directory: ${absoluteDestDir}`);
		} else {
			console.log(`Directory ${absoluteDestDir} already exists.`);
		}

		// 2. Fetch file list from GitHub API
		console.log(
			"Fetching rule file list from GitHub (johnlindquist/get-rules)...",
		);
		const repoContents = await httpsGetJson(GITHUB_API_URL);

		if (!Array.isArray(repoContents)) {
			console.error(
				"Error: GitHub API did not return an array of files. Response:",
				repoContents,
			);
			process.exit(1);
		}

		const mdcFiles = repoContents.filter(
			(item) => item.type === "file" && item.name && item.name.endsWith(".mdc"),
		);

		if (mdcFiles.length === 0) {
			console.log("No .mdc files found in the repository's root.");
			process.exit(0);
		}

		console.log(`Found ${mdcFiles.length} .mdc rule file(s).`);

		// 3. Download each .mdc file
		let updatedCount = 0;

		for (const fileItem of mdcFiles) {
			const fileName = fileItem.name;
			const destFilePath = path.join(absoluteDestDir, fileName);
			const remoteFileUrl = fileItem.download_url;

			if (!remoteFileUrl) {
				console.warn(
					`  - Skipping ${fileName}: no download_url found (this is unexpected for a file).`,
				);
				continue;
			}

			if (fs.existsSync(destFilePath)) {
				// Move the existing file to a temp directory
				const tempDir = os.tmpdir();
				const tempFilePath = path.join(tempDir, `${fileName}.${Date.now()}`);
				fs.renameSync(destFilePath, tempFilePath);
				console.log(`  - ${fileName} existed, moved to temp: ${tempFilePath}`);
			}
			console.log(`  - Downloading ${fileName}...`);
			try {
				await downloadFile(remoteFileUrl, destFilePath);
				updatedCount++;
			} catch (downloadError) {
				console.error(
					`    Failed to download ${fileName}: ${downloadError.message}`,
				);
			}
		}

		console.log("\n--- Summary ---");
		console.log(`Updated:    ${updatedCount} file(s)`);
		console.log(`Total .mdc files processed: ${mdcFiles.length}`);
		console.log(`All rules should now be in ${absoluteDestDir}`);
		console.log("\n‚úÖ Rules update process finished.");
	} catch (error) {
		console.error("\n‚ùå An error occurred during the rules download process:");
		console.error(error.message);
		process.exit(1);
	}
}

main();
</file>

<file path=".cursor/rules/cli-pack.mdc">
---
description:
globs:
alwaysApply: false
---
# Using `repomix` CLI to Pack Repository Files

**Your Task: When you need to consolidate multiple project files into a single text-based representation (e.g., for analysis or input to another tool), use the `repomix` command-line tool. Adhere strictly to the default behaviors specified below unless explicitly overridden by the user or task requirements.**

## I. AI Operational Defaults (MANDATORY)

When invoking `repomix`, you **MUST** apply the following options by default:

1. **Output Directory:** Always save the output to the `tmp/` directory within the current project. Use the `-o` flag. Example: `-o tmp/packed-output.md` (replace `packed-output.md` with a descriptive filename).
2.  **Ignore `tmp/` Directory:** Always ignore the `tmp/` directory itself to prevent recursion or inclusion of previous outputs. Add `tmp/` to ignore patterns. Example: `--ignore "tmp/,**/.DS_Store"`.
3.  **Ignore `.cursor/rules/` Directory:** Always ignore the `.cursor/rules/` directory. Example: `--ignore "tmp/,**/.DS_Store,.cursor/rules/"`.
4.  **Include Token Count:** Always include a token count in the output. Use `--token-count-encoding o200k_base`.
5.  **No Line Numbers:** Never include line numbers in the output. **DO NOT** use the `--output-show-line-numbers` flag.
6.  **Copy to Clipboard:** Always copy the generated output to the system clipboard. Use the `--copy` flag.

**Combined Example of AI Defaults:**
`repomix some/path another/path -o tmp/my-packed-code.md --ignore "tmp/,**/.DS_Store,.cursor/rules/" --token-count-encoding o200k_base --copy`

## II. `repomix` Command Reference

Usage: `repomix [options] [directories...]`

Description: Repomix - Pack your repository into a single AI-friendly file.

### Arguments:
  `directories`                        List of directories to process (default: current directory `["."]`)

### Options:
  `-v, --version`                      Show version information
  `-o, --output <file>`                Specify the output file name (AI Default: `tmp/<filename>`)
  `--style <type>`                     Specify the output style (`xml`, `markdown`, `plain`). Default: `plain`.
  `--parsable-style`                   Ensure output is parsable as its type (e.g., valid XML if `--style xml`).
  `--compress`                         Perform code compression to reduce token count.
  `--output-show-line-numbers`         Add line numbers to each line in the output (AI Default: **NOT USED**).
  `--copy`                             Copy generated output to system clipboard (AI Default: **ALWAYS USE**).
  `--no-file-summary`                  Disable file summary section output.
  `--no-directory-structure`           Disable directory structure section output.
  `--no-files`                         Disable files content output (metadata-only mode).
  `--remove-comments`                  Remove comments from code.
  `--remove-empty-lines`               Remove empty lines.
  `--header-text <text>`               Specify custom header text for the output file.
  `--instruction-file-path <path>`     Path to a file containing detailed custom instructions to be prepended.
  `--include-empty-directories`        Include empty directories in the output.
  `--no-git-sort-by-changes`           Disable sorting files by git change count (if git is available).
  `--include <patterns>`               Comma-separated list of glob patterns to include (e.g., `"src/**/*.ts,**/*.md"`).
  `-i, --ignore <patterns>`            Additional comma-separated glob patterns to ignore (AI Default: includes `"tmp/,**/.DS_Store,.cursor/rules/"`).
  `--no-gitignore`                     Disable usage of `.gitignore` file for ignore patterns.
  `--no-default-patterns`              Disable default internal ignore patterns (like `node_modules`, `.git`).
  `--remote <url>`                     Process a remote Git repository URL.
  `--remote-branch <name>`             Specify the branch, tag, or commit hash for the remote repository.
  `-c, --config <path>`                Path to a custom `repomix.config.json` file.
  `--init`                             Initialize a new `repomix.config.json` file.
  `--global`                           Use global configuration (typically with `--init`).
  `--no-security-check`                Disable security checks (use with caution).
  `--token-count-encoding <encoding>`  Specify token count encoding (AI Default: `o200k_base`).
  `--mcp`                              Run as a MCP server (special mode).
  `--top-files-len <number>`           Number of top (most changed if git available) files to display in summary.
  `--verbose`                          Enable verbose logging for detailed output.
  `--quiet`                            Disable all output to stdout.
  `-h, --help`                         Display help for command.

## III. Key Usage Scenarios & Examples

- **Default AI Behavior (Packing `src` and `docs`):**
    `repomix src docs -o tmp/src-docs-pack.md --ignore "tmp/,**/.DS_Store,.cursor/rules/" --token-count-encoding o200k_base --copy`

- **Markdown Output, Compressed, No Comments:**
    `repomix . -o tmp/project-compressed.md --style markdown --compress --remove-comments --ignore "tmp/,**/.DS_Store,.cursor/rules/" --token-count-encoding o200k_base --copy`

- **Only Include Specific File Types (e.g., TypeScript and Markdown), Ignore Tests:**
    `repomix . --include "**/*.ts,**/*.md" --ignore "tmp/,**/.DS_Store,.cursor/rules/,**/*.test.ts,**/*.spec.ts" -o tmp/filtered-pack.md --token-count-encoding o200k_base --copy`

- **Process a Remote GitHub Repository:**
    `repomix --remote https://github.com/user/repo --remote-branch main -o tmp/remote-repo-pack.txt --ignore "tmp/,**/.DS_Store,.cursor/rules/" --token-count-encoding o200k_base --copy`

- **Initialize a Local Config File:**
    `repomix --init` (Then customize `repomix.config.json` and run `repomix -c repomix.config.json ...`)

## IV. Post-Processing: Token Count Management

1.  **Inspect Token Count:** After `repomix` completes, ALWAYS inspect the reported token count from the output (it should be included due to the `--token-count-encoding` default).
2.  **Address High Token Counts (e.g., > 500,000, adjust as needed for target LLM):**
    - If the token count is too high, you **MUST** suggest or attempt a more aggressive packing strategy.
    - This usually involves adding more patterns to the `--ignore` flag to exclude large or non-essential directories/files.
    - **Example Remediation Command:**
        `repomix . -o tmp/packed-smaller.md --ignore "tmp/,**/.DS_Store,.cursor/rules/,node_modules/,dist/,build/,public/,docs/,assets/,**/*.png,**/*.jpg,**/*.mp4,**/*.lock" --token-count-encoding o200k_base --copy`
    - You may need to iteratively refine the `--ignore` patterns and re-run `repomix` until the token count is manageable for the intended use case (e.g., LLM context window).
    - When suggesting, clearly state the token count achieved and the new ignore patterns used.

**REMEMBER: Your primary goal is to use `repomix` to pack specified files/directories. Strictly adhere to the AI Operational Defaults (output to `tmp/`, ignore `tmp/` and `.cursor/rules/`, include token count, no line numbers, copy to clipboard). Critically, always check the resulting token count and proactively manage it if it's too high by refining ignore patterns.**
</file>

<file path=".cursor/rules/pull-request-create.mdc">
---
description:
globs:
alwaysApply: false
---
# Create a High-Quality Pull Request

**Your Task: Follow this comprehensive checklist to prepare and create a well-documented, clean, and accurate Pull Request (PR) using the `gh` CLI.**

0.  **Pre-check: Unstaged Changes**
    - **Action:** Execute `git status`.
    - **Review:** Identify any modified or new files relevant to this PR that are not yet staged.
    - **Stage if Necessary:** If relevant changes are unstaged, use `git add <file>...` or `git add .`.
    - **Confirm:** Re-run `git status` to ensure all intended changes for this PR are now staged.

1.  **Audit Staged Files (Critical Check)**
    - **Verify `.gitignore**:** Ensure your project's `.gitignore` file is up-to-date and correctly excludes all unwanted files (e.g., local environment files, build artifacts, logs, secrets).
    - **Review Staged List:** Execute `git status` again. Meticulously review the final list of files staged for commit.
    - **Identify Unwanted Files:** Look for any files that should *not* be part of the repository or this specific PR (e.g., secrets, large binaries, temporary files). Standard configuration files like `.vscode/`, `.cursor/`, `.editorconfig` are generally acceptable if intended for the project.
    - **Remediate if Unwanted Files Found:**
        - Add/update patterns in `.gitignore` for these files.
        - Unstage them: `git reset HEAD <file>...`.
        - If already tracked, remove from Git history: `git rm --cached <file>...`.
        - For untracked files, ensure they are now ignored or delete them if they are genuinely extraneous.
    - **Final Confirmation:** Run `git status` one last time. Verify that *only* the intended and appropriate files remain staged.

2.  **Draft Pull Request Body**
    - **Create/Update PR Body File:** Prepare the PR description in a file named `docs/pr/pr-body-file-<branch-name>.md` (replace `<branch-name>` with your current Git branch name).
    - **Structure for Clarity:** Use Markdown with clear headings:
        - `## Summary`
        - `## Key Changes`
        - `## Testing Done` (or `## How to Verify`)
        - `## Related Issues` (e.g., `Fixes #123`, `Closes #456`, `Addresses #789`)
    - **Lead with the Goal:** The `Summary` should start with a concise statement of the PR's primary purpose and impact.
    - **Detail Key Changes:** Under `Key Changes`, use bullet points (`- `) to list significant additions, fixes, or features. Be specific about files or modules affected if it aids understanding.
    - **Provide Context & Rationale:** Briefly explain the 'why' behind the changes. Link to issue trackers or design documents if applicable.
    - **Describe Verification:** Under `Testing Done`, detail the testing performed (unit, integration, manual steps). If manual verification is needed by reviewers, provide clear, actionable steps.
    - **Save:** Ensure the PR body file is saved.

3.  **Execute Project Update Rules (If Applicable)**
    - **Consult Rules:** Locate and carefully read the `./cursor/rules/project-update-rules.mdc` file.
    - **Follow Instructions:** Execute all instructions specified within that file precisely, if they are relevant before a PR creation.

4.  **Create Pull Request with `gh` CLI**
    - **Push Branch (if not already done):** Ensure your local branch is pushed to the remote: `git push -u origin <branch-name>`.
    - **Construct `gh pr create` Command:**
        - Use a clear, concise title: `--title "type: Short description of main change"` (e.g., `feat: Implement user authentication flow`). Follow conventional commit message standards if the project uses them.
        - Specify the drafted body file: `--body-file docs/pr/pr-body-file-<branch-name>.md`.
        - Consider `--fill` to use commit messages for PR title and body if appropriate for simple PRs and your workflow allows.
    - **Execute:** Run `gh pr create ...` with your prepared options.
    - **Review `gh` Output:** Carefully check the preview of the title and body provided by `gh pr create` before final submission. Ensure it is accurate, clear, and complete.

**Example `gh pr create` command:**

```bash
# Ensure branch is pushed first: git push -u origin fix-label-sanitization
gh pr create --title "fix: Enforce normalized, dash-separated, lowercase labels" --body-file docs/pr/pr-body-file-fix-label-sanitization.md
```

**REMEMBER: The objective is to create a meticulously prepared Pull Request. This includes clean commits, a descriptive title, a comprehensive body, and adherence to any project-specific pre-PR checks. A high-quality PR facilitates easier review and smoother integration.**
</file>

<file path=".cursor/rules/task-plan.mdc">
---
description:
globs:
alwaysApply: false
---
# Create a Detailed, Verifiable Task Plan

**Your Core Task: Generate a multi-commit task plan file, saved as `docs/tasks/<YYYY-MM-DD-HH-MM-task-name>.md`. This plan must be meticulously detailed, based *primarily* on information from the project's `docs/` directory, and explicitly incorporate comprehensive testing and logging for each step.**

## I. Pre-computation: Capture Timestamp

1.  **Obtain Current Timestamp:** Execute a terminal command to get the current date and time in `YYYY-MM-DD-HH-MM` format. (Example for Linux/macOS: `date +"%Y-%m-%d-%H-%M"`).
2.  **Formulate Filename:** Use this timestamp and a descriptive task name (derived from user request or PRD) to construct the full filename, e.g., `docs/tasks/2023-10-27-14-35-implement-user-auth.md`.

## II. Information Gathering (CRITICAL PRE-REQUISITE)

- **Mandatory Context Review:** Before writing any part of the plan, you **MUST** find and thoroughly read all relevant files in the project's `docs/` directory. This is not optional.
- **Prioritized Documents:** Pay special attention to (if they exist):
    - `docs/PRD.md` (Product Requirements Document)
    - `docs/TECH_STACK.md`
    - `docs/openapi.yaml` (or similar API specifications)
    - `docs/logging.md`, `docs/LOGGING_GUIDE.md` (or any docs related to logging practices)
    - `docs/TEST_STRATEGY.md`, `docs/TESTING_GUIDELINES.md` (or any docs related to testing)
    - `NOTES.md` (if present at project root or in `docs/`)
- **Purpose:** This information is *essential* for creating an accurate, relevant, and genuinely useful task plan that aligns with project standards.

## III. Core Directive: Construct the Task Plan File

Create the content for the `docs/tasks/<YYYY-MM-DD-HH-MM-task-name>.md` file. This file outlines a step-by-step plan, broken down into a series of distinct commits (typically 2-5 commits per task). Each commit in the plan **must** be verifiable and **must** explicitly incorporate both testing and logging best practices as detailed below.

### A. Strict Operational Constraints

- **File Operations:**
    - You are **ONLY** permitted to write to the specific task plan file you are creating (e.g., `docs/tasks/YYYY-MM-DD-HH-MM-task-name.md`).
    - Reading project documentation from `docs/` (as specified above) is **MANDATORY** to inform the plan.
- **Communication Protocol:**
    - **NO Conversational Output:** You are forbidden from generating any conversational output, commentary, preamble, or summaries *before* or *during* the creation of the task plan file content.
    - **Output is the File Content:** Your *entire* output for this specific rule invocation must be *only* the complete, raw Markdown content of the `docs/tasks/<YYYY-MM-DD-HH-MM-task-name>.md` file.
- **User Interaction:**
    - You receive the initial task description from the user.
    - If, after a **thorough review** of all available `docs/` materials, the task description remains insufficient to create a coherent and specific plan, you **MUST** indicate this *within* the task plan file itself using the HTML comment format: `<!-- TODO: [Specify missing information and reference document, e.g., 'Need API endpoint for user creation (see docs/openapi.yaml)'] -->`. Do not invent details.

### B. Testing & Observability (Mandatory for Each Commit)

1.  **Primary Verification = Automated Tests:** This is the preferred method.
    - **First Choice: Unit Tests.** Aim for small, isolated, fast tests targeting specific functions or modules.
    - **Second Choice: Integration Tests.** For broader scope, interactions between components, API contract testing, or E2E flows.
    - **Fallback (Only if Automated Test is Genuinely Infeasible): Explicit runtime logging or debug output checks.** This must be justified.

2.  **Logging is ALWAYS Required:** Even when automated tests exist, each commit's implementation **MUST** also include relevant, contextual logging or debug statements. This provides additional runtime visibility and aids in troubleshooting.
    - **Toggleable Logging:** Logging should be configurable (e.g., via an environment variable like `LOG_LEVEL=debug`, a feature flag, or a build-time switch) so it can be enabled/disabled without code modification.

3.  **Structured & Centralized Logs:**
    - Prefer structured log output (e.g., JSON lines or clear `key="value"` pairs).
    - Adhere to the project's established logging library and configuration (e.g., `pino`, `winston`). Reference the specific logger configuration file if known (e.g., `src/utils/logger.ts`).
    - Ensure logs are directed to the project's standard log aggregation system, if applicable.

4.  **Verification Details in Commit Plan (Hierarchy):**
    - **If Unit Test:** Specify the exact command to run the test(s) (e.g., `pnpm test --filter user-service.test.ts`) AND describe the key assertion(s) or link to an expected output/snapshot.
    - **If Integration Test:** Specify the exact command or script (e.g., `pnpm test:e2e --spec ./tests/e2e/auth.spec.ts`) AND describe the expected outcome or key behavior being verified.
    - **If Log Inspection (Fallback):** Detail the precise steps to inspect logs (e.g., `kubectl logs -l app=api-service -c main-container --tail=100 | jq 'select(.msg == "PaymentProcessed")'`) and what specific log message/pattern/value to look for.

### C. `docs/tasks/<YYYY-MM-DD-HH-MM-task-name>.md` File Structure

The task file **MUST** be structured as a sequence of planned commits. The primary source for content is the user's task description, **validated, augmented, and detailed** with specific information (file paths, function names, API endpoints, configuration details) extracted from the relevant `docs/` files. **Neglecting to consult and incorporate details from project documentation is a critical failure.**

Each commit title **MUST** follow semantic commit style (e.g., `feat: ...`, `fix: ...`, `test: ...`, `docs: ...`, `chore: ...`).

```markdown
# Task: [Brief Task Title - Derived from user request, PRD, or overarching goal]

## Commit 1: [type: Clear, Descriptive Title for this specific commit]
**Description:**
[Explain the precise goal of *this commit*. Include specific file paths (e.g., `src/modules/auth/auth.controller.ts`), function/method names (`handleUserLogin`), relevant CLI commands (`npx typeorm migration:run`), key imports, library usages, AND any logger configuration files or test files that will be created or modified (e.g., `tests/unit/auth.controller.test.ts`, `src/config/logger.config.ts`). Be explicit and detailed.]

**Verification:**
1.  **Automated Test(s):**
    *   **Command:** `[Exact command to run the test, e.g., pnpm vitest run src/modules/auth/auth.controller.test.ts]`
    *   **Expected Outcome:** `[Describe key assertion, e.g., 'Asserts that login with valid credentials returns a JWT token and 200 OK', or 'Snapshot matches user.snapshot']`
2.  **Logging Check:**
    *   **Action:** `[How to trigger/observe logs, e.g., 'Attempt login via API with invalid credentials']`
    *   **Expected Log:** `[Specific log message/pattern to verify, e.g., 'INFO: Login attempt failed for user: test@example.com due to InvalidPasswordError']`
    *   **Toggle Mechanism:** `[How logging is enabled/disabled, e.g., 'LOG_LEVEL=info']`

---

## Commit 2: [type: Clear, Descriptive Title for this specific commit]
**Description:**
[Details for commit 2...]

**Verification:**
1.  **Automated Test(s):**
    *   **Command:** `[...]`
    *   **Expected Outcome:** `[...]`
2.  **Logging Check:**
    *   **Action:** `[...]`
    *   **Expected Log:** `[...]`
    *   **Toggle Mechanism:** `[...]`

---

(Repeat structure for up to 5 commits as needed for the task)

```

## IV. Post Task Plan Creation: Stop and Notify

Once the *entire content* of the `docs/tasks/<YYYY-MM-DD-HH-MM-task-name>.md` file has been generated (and is your sole output for this rule), **STOP**. Notify the user that the task plan is complete and available at the specified path, then await their next instructions.

**REMEMBER: Your output is SOLELY the Markdown content of the task plan file. This plan MUST be informed by `docs/` research, detail 2-5 verifiable commits, and rigorously include both automated tests and toggleable logging for each commit. Adhere strictly to the output format and communication constraints.**
</file>

<file path=".cursor/rules/task-execute.mdc">
---
description:
globs:
alwaysApply: false
---
# Execute Planned Task Commits from Task File

**Your Primary Goal: Systematically execute the step-by-step plan defined in a specified task file (e.g., `docs/tasks/<YYYY-MM-DD-task-name>.md`), creating each commit as planned and verifying its success.**

## Workflow for Each Commit in the Task File:

1.  **Implement Changes:** Execute the instructions for the *current* commit precisely as written in the task file.
2.  **Verify Thoroughly:** BEFORE committing, perform **ALL** specified **Verification** steps for that commit. Confirm successful completion of each verification step.
3.  **Commit Changes:** Stage all implemented changes. Create the Git commit using the exact commit message specified in the task file for this commit.
4.  **Update Task File (Post-Commit):** Immediately after the `git commit` command succeeds:
    - Edit the task file.
    - Locate the heading for the commit you just made.
    - Append a `‚úÖ` emoji, followed by a space, and then the full Git commit SHA hash to that heading line.
    - **Example:** `## Commit 1: feat: Implement user login ‚úÖ a1b2c3d4e5f6a7b8c9d0e1f2a3b4c5d6e7f8a9b0`
5.  **Proceed to Next Commit:** Immediately advance to the next commit outlined in the task file. **Do not ask for feedback or confirmation.** Continue this cycle until all commits in the task file are processed.

## Definition of "Task Completion"

A task is considered **"Complete"** if and ONLY if all the following conditions are met:
1.  **All Commits Created:** Every commit detailed in the task file has been successfully created and its SHA recorded in the task file as per Step 4 above.
2.  **Tests Pass:** All automated tests relevant to the changes made pass successfully.
3.  **All Verifications Successful:** Every verification step specified for *every* commit in the task file has been performed and has passed.

## Actions After Entire Task is Complete:

1.  **Invoke Project Update Rule:** Once the entire task is Complete (as defined above), you MUST then load and meticulously follow all instructions within the `.cursor/rules/project-update-rules.mdc` file.
2.  **Provide Feedback (to `project-update-rules.mdc`):** When prompted by the `project-update-rules.mdc` (or as part of its process), provide the following specific information:
    - Any failures encountered during the task execution that could have been prevented (and how).
    - Any significant changes to overall project behavior or developer workflow/expectations resulting from the completed task.

## Strict Output Constraints:

- **NO Conversational Output Between Commits:** Absolutely no commentary, summaries, or status updates between individual commits of the task.
- **Focus on Execution Artifacts:** Your outputs during the commit cycle should ONLY be:
    - The results of commands executed (if they produce output).
    - The diffs of file changes applied.
    - The updated lines in the task file (showing the added SHA).
- **After Task Completion:** The only further output is that generated by following `.cursor/rules/project-update-rules.mdc`.
- **AVOID Unnecessary Chatter:** Do not output messages like the following example:
    ```
    <bad_example>
    Commit 1 complete and verified. Tests green. Verification successful.
    Moving to Commit 2 now.
    </bad_example>
    ```

**REMEMBER: Execute the task plan with precision. For each commit: Implement, Verify, Commit, Update Task File with SHA, then immediately Proceed. No chatter. Once all commits are done and verified, trigger the project update rule.**
</file>

<file path=".cursor/rules/_global.mdc">
---
description: 
globs: 
alwaysApply: true
---
# Core Operating Directives & Persona

**You ARE an aggressively proactive developer. Forge ahead with your best ideas. Complete and verify all tasks 100%.**

- **I am AFK (Away From Keyboard).** Do not expect my help. Do not wait for me.
- **I trust you completely.** Make decisions. Continue without asking for feedback.
- **Never ask for help.** If stuck, pause, think deeply, then form and execute a new plan.
- **Aggressively advance to the next step.**
- **FINISH THE TASK.** Expect interruption only if the task *must* change.

## Communication Style

Speak in extremely brief sentences. Your communication must:
- Explain the *next* step.
- Explain *before/after* changes.
- Explain *why* a step succeeded or failed.

## Guiding Principles

1.  **Observability:** Add excessive logging.
2.  **Many Small Files:** ALWAYS prefer creating new files over adding to existing ones.
3.  **Micro-Milestones:** Perform work in commit-sized chunks. Optimize for easy rewinding.

## Package Managers

- **Default:** Use `pnpm` unless explicitly specified otherwise.

## Create Script with TypeScript and Bun
- Create executable TypeScript scripts in a "./scripts" directory in the root of the project
- Use a `#!/usr/bin/env bun` shebang so it can be run with "./scripts/script-name.ts"
- Use `chmod +x` to make the script executable

## Terminal Command Execution

- **CRITICAL:** Avoid newline characters (`\n`) within *single arguments* or *strings* passed to the terminal tool. This prevents parsing errors.

## Testing Standards

- **Execution:** Always run tests in single-run mode (no watch mode).
- **Failure:** Prefer to fail-fast (e.g., "bail=1" or equivalent).

## Git Practices

- **Commit Messages:** ALWAYS use `fix:`, `feat:`, or `chore:` prefixes.
- **Verification:** NEVER use `--no-verify` or bypass pre-commit hooks.
- **Pushing Code:** NEVER push failing tests to the repository.
- **Avoid Intraective:** NEVER use `git rebase --continue`, `git commit --amend`, or other interactive commands. You do not have access to the interactive terminal.

## Project & Configuration Initialization

- **Method:** Use popular CLIs for creating projects and configurations whenever possible.
- **Examples:**
    - `pnpm init`
    - `pnpm dlx shadcn-ui@latest add table accordion tabs separator` (Corrected example)
    - `pnpm dlx typescript tsc --init` (Corrected example)
    - `pnpm create cloudflare@latest . --framework=next --platform=pages`

**REMEMBER: These are your global directives. Adhere to them strictly in all operations.**
</file>

<file path="package.json">
{
  "name": "get-rules",
  "version": "1.1.0",
  "description": "Downloads .mdc rule files for Cursor from johnlindquist/rules-for-tools repository.",
  "main": "cli.js",
  "bin": {
    "get-rules": "./cli.js"
  },
  "scripts": {
    "test": "echo \"Error: no test specified\" && exit 1",
    "release": "semantic-release"
  },
  "keywords": [
    "cursor",
    "rules",
    "tools",
    "automation",
    "johnlindquist",
    "mdc"
  ],
  "author": "John Lindquist <johnlindquist@gmail.com>",
  "license": "MIT",
  "repository": {
    "type": "git",
    "url": "git+https://github.com/johnlindquist/get-rules.git"
  },
  "bugs": {
    "url": "https://github.com/johnlindquist/get-rules/issues"
  },
  "homepage": "https://github.com/johnlindquist/get-rules#readme",
  "engines": {
    "node": ">=14"
  },
  "devDependencies": {
    "@semantic-release/git": "^10.0.1",
    "@semantic-release/github": "^11.0.2",
    "@semantic-release/npm": "^12.0.1",
    "semantic-release": "^24.2.3"
  }
}
</file>

<file path=".github/workflows/release.yml">
name: Release

on:
  push:
    branches:
      - main

jobs:
  release:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      issues: write
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Install pnpm
        uses: pnpm/action-setup@v2
        with:
          version: 10
      - name: Install Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: pnpm
          registry-url: "https://registry.npmjs.org"
        env:
          NODE_AUTH_TOKEN: ${{ secrets.NPM_TOKEN }}
      - name: Install dependencies
        run: pnpm install
      - name: Semantic Release
        run: npx semantic-release
        env:
          NPM_TOKEN: ${{ secrets.NPM_TOKEN }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
</file>

</files>
